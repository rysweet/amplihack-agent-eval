{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"amplihack-agent-eval","text":"<p>Evaluation framework for goal-seeking AI agents. Tests memory recall, tool use, planning, and reasoning across progressive difficulty levels (L1-L12).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Long-horizon memory stress tests -- Generates 1000+ turn dialogues with embedded facts, then quizzes the agent on details from various points in the conversation</li> <li>Hybrid grading -- Deterministic (rubric keywords) + LLM (semantic judgment) with multi-vote stability</li> <li>Progressive difficulty levels -- L1 (simple recall) through L12 (far transfer reasoning)</li> <li>Agent-agnostic -- Works with any agent through the <code>AgentAdapter</code> interface</li> <li>Self-improvement loop -- Automated EVAL -&gt; ANALYZE -&gt; PROPOSE -&gt; CHALLENGE -&gt; VOTE -&gt; APPLY -&gt; RE-EVAL cycle</li> <li>Multi-seed holdout -- Run across multiple random seeds to measure inter-seed variance</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation (data generation and adapters, no LLM grading)\npip install amplihack-agent-eval\n\n# With Anthropic grading support\npip install amplihack-agent-eval[anthropic]\n\n# Development\npip install amplihack-agent-eval[dev]\n\n# Everything\npip install amplihack-agent-eval[all,dev]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-implement-the-agentadapter-interface","title":"1. Implement the AgentAdapter interface","text":"<pre><code>from amplihack_eval import AgentAdapter, AgentResponse\n\nclass MyMemoryAgent(AgentAdapter):\n    def __init__(self):\n        self.memory = []\n\n    def learn(self, content: str) -&gt; None:\n        self.memory.append(content)\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        relevant = [m for m in self.memory\n                    if any(w in m.lower() for w in question.lower().split())]\n        return AgentResponse(\n            answer=\" \".join(relevant[:3]) if relevant else \"I don't know\"\n        )\n\n    def reset(self) -&gt; None:\n        self.memory.clear()\n\n    def close(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"#2-run-an-evaluation","title":"2. Run an evaluation","text":"<pre><code>from amplihack_eval import EvalRunner\n\nagent = MyMemoryAgent()\nrunner = EvalRunner(num_turns=100, num_questions=20, grader_votes=3)\nreport = runner.run(agent)\n\nprint(f\"Overall score: {report.overall_score:.2%}\")\nfor cb in report.category_breakdown:\n    print(f\"  {cb.category}: {cb.avg_score:.2%}\")\n</code></pre>"},{"location":"#3-cli-usage","title":"3. CLI usage","text":"<pre><code># Run eval against an HTTP agent\namplihack-eval run --turns 100 --questions 20 --adapter http --agent-url http://localhost:8000\n\n# Run eval with amplihack's LearningAgent\namplihack-eval run --turns 100 --questions 20 --adapter learning-agent\n\n# Multi-seed comparison\namplihack-eval compare --seeds 42,123,456,789 --turns 100\n\n# Self-improvement loop\namplihack-eval self-improve --iterations 5 --turns 100\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"Guide Description Architecture Package layout, core concepts, and design principles Evaluation Levels Complete guide to all 12 progressive difficulty levels (L1-L12) Writing Adapters How to write custom <code>AgentAdapter</code> implementations Self-Improvement Loop Automated improvement cycle with safety gates Multi-Agent Eval Planned multi-agent evaluation scenarios"},{"location":"#environment-variables","title":"Environment Variables","text":"Variable Purpose Default <code>ANTHROPIC_API_KEY</code> Required for LLM grading -- <code>GRADER_MODEL</code> Model for grading <code>claude-sonnet-4-5-20250929</code> <code>EVAL_MODEL</code> Model for LearningAgent adapter <code>claude-sonnet-4-5-20250929</code>"},{"location":"#contributing","title":"Contributing","text":"<pre><code># Clone the repository\ngit clone https://github.com/rysweet/amplihack-agent-eval.git\ncd amplihack-agent-eval\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest tests/ -q\n\n# Run linting\nruff check src/ tests/\nruff format --check src/ tests/\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"adapters/","title":"Writing Custom Agent Adapters","text":""},{"location":"adapters/#the-agentadapter-interface","title":"The AgentAdapter Interface","text":"<p>To make any agent evaluable, implement the <code>AgentAdapter</code> abstract base class from <code>amplihack_eval.adapters.base</code>:</p> <pre><code>from amplihack_eval import AgentAdapter, AgentResponse, ToolCall\n\nclass MyAgent(AgentAdapter):\n    def learn(self, content: str) -&gt; None:\n        \"\"\"Feed content to the agent for learning/memorization.\"\"\"\n        ...\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        \"\"\"Ask the agent a question. Returns answer + trajectory.\"\"\"\n        ...\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset agent state between eval runs.\"\"\"\n        ...\n\n    def close(self) -&gt; None:\n        \"\"\"Clean up resources (connections, files, etc.).\"\"\"\n        ...\n</code></pre>"},{"location":"adapters/#required-methods","title":"Required Methods","text":"Method Signature Purpose <code>learn</code> <code>(content: str) -&gt; None</code> Feed content to the agent. Called once per dialogue turn during evaluation. <code>answer</code> <code>(question: str) -&gt; AgentResponse</code> Ask the agent a question. Must return an <code>AgentResponse</code>. <code>reset</code> <code>() -&gt; None</code> Reset all agent state. Called between evaluation runs. <code>close</code> <code>() -&gt; None</code> Clean up resources. Called when evaluation is complete."},{"location":"adapters/#optional-properties","title":"Optional Properties","text":"Property Type Default Purpose <code>capabilities</code> <code>set[str]</code> <code>{\"memory\"}</code> Declare what the agent can do. Used by the runner to select appropriate eval levels. <code>name</code> <code>str</code> Class name Human-readable name for reports and logs."},{"location":"adapters/#agentresponse","title":"AgentResponse","text":"<p>The <code>answer()</code> method must return an <code>AgentResponse</code>:</p> <pre><code>@dataclass\nclass AgentResponse:\n    answer: str                              # Required: the agent's answer text\n    tool_calls: list[ToolCall] = []          # Optional: tool invocations\n    reasoning_trace: str = \"\"                # Optional: chain-of-thought\n    confidence: float = 0.0                  # Optional: self-reported confidence\n    metadata: dict[str, Any] = {}            # Optional: arbitrary metadata\n</code></pre>"},{"location":"adapters/#toolcall","title":"ToolCall","text":"<p>If your agent uses tools, capture them for trajectory analysis:</p> <pre><code>@dataclass\nclass ToolCall:\n    tool_name: str               # Name of the tool invoked\n    arguments: dict[str, Any]    # Arguments passed to the tool\n    result: str                  # String result from the tool\n    timestamp: float = 0.0       # Optional: when the call happened\n</code></pre>"},{"location":"adapters/#built-in-adapters","title":"Built-in Adapters","text":""},{"location":"adapters/#httpadapter","title":"HttpAdapter","text":"<p>For agents exposed via REST API:</p> <pre><code>from amplihack_eval.adapters.http_adapter import HttpAdapter\n\nadapter = HttpAdapter(\n    base_url=\"http://localhost:8000\",\n    timeout=30,\n)\n</code></pre> <p>Expected endpoints: - <code>POST /learn</code> with <code>{\"content\": \"...\"}</code> -&gt; 200 OK - <code>POST /answer</code> with <code>{\"question\": \"...\"}</code> -&gt; <code>{\"answer\": \"...\", \"tool_calls\": [...], ...}</code> - <code>POST /reset</code> -&gt; 200 OK</p>"},{"location":"adapters/#subprocessadapter","title":"SubprocessAdapter","text":"<p>For agents invokable via CLI:</p> <pre><code>from amplihack_eval.adapters.subprocess_adapter import SubprocessAdapter\n\nadapter = SubprocessAdapter(\n    command=[\"python\", \"my_agent.py\"],\n    learn_flag=\"--learn\",\n    answer_flag=\"--answer\",\n)\n</code></pre>"},{"location":"adapters/#learningagentadapter","title":"LearningAgentAdapter","text":"<p>For the amplihack LearningAgent (requires <code>amplihack</code> package):</p> <pre><code>from amplihack_eval.adapters.learning_agent import LearningAgentAdapter\n\nadapter = LearningAgentAdapter()\n</code></pre>"},{"location":"adapters/#complete-custom-adapter-example","title":"Complete Custom Adapter Example","text":"<pre><code>from amplihack_eval import AgentAdapter, AgentResponse, ToolCall\n\nclass RAGAgent(AgentAdapter):\n    \"\"\"Adapter for a retrieval-augmented generation agent.\"\"\"\n\n    def __init__(self, db_url: str, model: str = \"gpt-4\"):\n        self.db_url = db_url\n        self.model = model\n        self.client = VectorDBClient(db_url)\n        self.llm = LLMClient(model)\n\n    def learn(self, content: str) -&gt; None:\n        # Chunk and embed content into vector DB\n        chunks = self._chunk(content)\n        embeddings = self.llm.embed(chunks)\n        self.client.upsert(chunks, embeddings)\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        # Retrieve relevant chunks\n        query_embedding = self.llm.embed([question])[0]\n        results = self.client.search(query_embedding, top_k=5)\n\n        # Generate answer with context\n        context = \"\\n\".join(r.text for r in results)\n        response = self.llm.generate(\n            f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n        )\n\n        return AgentResponse(\n            answer=response.text,\n            tool_calls=[\n                ToolCall(\n                    tool_name=\"vector_search\",\n                    arguments={\"query\": question, \"top_k\": 5},\n                    result=f\"Found {len(results)} chunks\",\n                )\n            ],\n            reasoning_trace=f\"Retrieved {len(results)} chunks, generated answer\",\n            confidence=response.confidence,\n        )\n\n    def reset(self) -&gt; None:\n        self.client.clear()\n\n    def close(self) -&gt; None:\n        self.client.close()\n        self.llm.close()\n\n    @property\n    def capabilities(self) -&gt; set[str]:\n        return {\"memory\", \"tool_use\"}\n\n    @property\n    def name(self) -&gt; str:\n        return f\"RAGAgent({self.model})\"\n</code></pre>"},{"location":"adapters/#running-evaluation-with-a-custom-adapter","title":"Running Evaluation with a Custom Adapter","text":"<pre><code>from amplihack_eval import EvalRunner\n\nagent = RAGAgent(db_url=\"http://localhost:6333\", model=\"gpt-4\")\nrunner = EvalRunner(num_turns=100, num_questions=20, grader_votes=3)\nreport = runner.run(agent)\n\nprint(f\"Overall: {report.overall_score:.2%}\")\nfor cb in report.category_breakdown:\n    print(f\"  {cb.category}: {cb.avg_score:.2%}\")\n\nagent.close()\n</code></pre>"},{"location":"adapters/#tips","title":"Tips","text":"<ul> <li>Keep <code>learn()</code> fast: The runner calls it once per dialogue turn (potentially 1000+ times). Batch operations if possible.</li> <li>Capture tool calls: Even if your agent does not use explicit tools, logging internal retrieval as a <code>ToolCall</code> enables richer analysis.</li> <li>Set confidence: If your agent can estimate confidence, include it. The grader uses confidence calibration in advanced eval levels (L8).</li> <li>Reset completely: <code>reset()</code> must clear ALL state. Leftover state between runs corrupts multi-seed evaluation.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p><code>amplihack-agent-eval</code> is a modular evaluation framework for goal-seeking AI agents. It tests memory recall, tool use, planning, and reasoning across progressive difficulty levels (L1-L12).</p> <p>The package follows a layered architecture with clear separation of concerns:</p> <pre><code>                  +------------------+\n                  |      CLI         |  amplihack_eval.cli\n                  +--------+---------+\n                           |\n              +------------+------------+\n              |                         |\n     +--------v---------+    +---------v----------+\n     |   EvalRunner      |    |  SelfImproveRunner |\n     |   (core/runner)   |    |  (self_improve/)   |\n     +--------+----------+    +---------+----------+\n              |                         |\n     +--------v---------+              |\n     |   Grader          |&lt;-------------+\n     |   (core/grader)   |\n     +--------+----------+\n              |\n     +--------v---------+\n     |   Data Generation |\n     |   (data/)         |\n     +-------------------+\n\n     +-------------------+\n     |   AgentAdapter    |  adapters/base.py (interface)\n     +--------+----------+\n              |\n     +--------+----------+----------+\n     |        |          |          |\n   HTTP   Subprocess  Learning   Custom\n   Adapter  Adapter   Agent      (yours)\n</code></pre>"},{"location":"architecture/#package-layout","title":"Package Layout","text":"<pre><code>src/amplihack_eval/\n    __init__.py              # Public API exports + __version__\n    py.typed                 # PEP 561 type checking marker\n    cli.py                   # CLI entry point (amplihack-eval command)\n\n    adapters/                # Agent adapter layer\n        base.py              # AgentAdapter ABC, AgentResponse, ToolCall\n        http_adapter.py      # REST API adapter (POST /learn, /answer, /reset)\n        subprocess_adapter.py # CLI subprocess adapter\n        learning_agent.py    # amplihack LearningAgent adapter\n\n    core/                    # Evaluation engine\n        runner.py            # EvalRunner: long-horizon memory stress test\n        grader.py            # Hybrid deterministic + LLM semantic grading\n        multi_seed.py        # Multi-seed holdout evaluation\n\n    data/                    # Test data generation\n        long_horizon.py      # 5000-turn dialogue generator\n        progressive_levels.py # L1-L12 level definitions (articles + questions)\n\n    levels/                  # Convenience re-exports of level definitions\n        __init__.py          # Re-exports ALL_LEVELS, get_level_by_id, etc.\n\n    self_improve/            # Automated self-improvement loop\n        runner.py            # SelfImproveRunner: EVAL-&gt;ANALYZE-&gt;PROPOSE-&gt;VOTE-&gt;APPLY\n        patch_proposer.py    # LLM-powered patch generation\n        reviewer_voting.py   # 3-reviewer A/B voting system\n\n    multi_agent_eval/        # Multi-agent scenarios (future)\n        __init__.py\n</code></pre>"},{"location":"architecture/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/#agentadapter-adaptersbasepy","title":"AgentAdapter (adapters/base.py)","text":"<p>The central abstraction. Any agent that implements <code>learn()</code>, <code>answer()</code>, <code>reset()</code>, and <code>close()</code> can be evaluated. Adapters capture tool call trajectories and reasoning traces for deeper analysis.</p>"},{"location":"architecture/#evalrunner-corerunnerpy","title":"EvalRunner (core/runner.py)","text":"<p>Generates a long-horizon dialogue (100-5000 turns), feeds it to the agent via <code>learn()</code>, then quizzes the agent with generated questions. Each answer is graded on multiple dimensions using the hybrid grader.</p> <p>Key types: - <code>EvalResult</code>: Per-question result with dimension scores - <code>EvalReport</code>: Aggregate report with category breakdown - <code>DimensionScore</code>: Score on a single grading dimension - <code>CategoryBreakdown</code>: Average scores per question category</p>"},{"location":"architecture/#grader-coregraderpy","title":"Grader (core/grader.py)","text":"<p>Two-stage grading: 1. Deterministic: Required keywords, acceptable paraphrases, incorrect pattern detection 2. LLM Semantic: When deterministic grading is insufficient, uses LLM to evaluate semantic correctness</p> <p>Multi-vote stability: Grade N times, take median to reduce LLM noise.</p>"},{"location":"architecture/#data-generation-data","title":"Data Generation (data/)","text":"<ul> <li><code>long_horizon.py</code>: Generates synthetic dialogues with embedded facts across 12 topic blocks. Facts are tracked in a <code>GroundTruth</code> structure for question generation.</li> <li><code>progressive_levels.py</code>: Hand-crafted test levels L1-L12 with articles and questions of increasing cognitive complexity.</li> </ul>"},{"location":"architecture/#self-improvement-loop-self_improve","title":"Self-Improvement Loop (self_improve/)","text":"<p>Automated improvement cycle: 1. EVAL: Run evaluation, get per-category scores 2. ANALYZE: Identify worst-performing category 3. PROPOSE: LLM generates a patch hypothesis 4. CHALLENGE: Devil's advocate reviews the proposal 5. VOTE: 3 reviewers vote accept/reject 6. APPLY: If accepted, apply the patch 7. RE-EVAL: Run evaluation again to measure impact 8. DECIDE: If regression, auto-revert; if improvement, keep</p>"},{"location":"architecture/#multi-seed-evaluation-coremulti_seedpy","title":"Multi-Seed Evaluation (core/multi_seed.py)","text":"<p>Runs the same evaluation across multiple random seeds (default: 42, 123, 456, 789) to measure inter-seed variance. Questions with &gt;10 percentage point variance are flagged as noisy.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ol> <li>Agent-agnostic: The <code>AgentAdapter</code> interface makes any agent evaluable</li> <li>Deterministic data: Same seed produces identical dialogues and questions</li> <li>Hybrid grading: Deterministic rubrics + LLM semantic judgment</li> <li>Progressive difficulty: L1 (simple recall) through L12 (far transfer)</li> <li>Safety: Self-improvement never modifies grader, test data, or safety constraints</li> <li>Reproducibility: All results are logged with full configuration</li> </ol>"},{"location":"levels/","title":"Evaluation Levels (L1-L12)","text":""},{"location":"levels/#overview","title":"Overview","text":"<p>The evaluation framework uses 12 progressive difficulty levels to test different cognitive capabilities of AI agents. Each level builds on previous ones, requiring increasingly sophisticated reasoning.</p>"},{"location":"levels/#level-categories","title":"Level Categories","text":""},{"location":"levels/#core-levels-l1-l6","title":"Core Levels (L1-L6)","text":"<p>These test fundamental memory and reasoning capabilities.</p>"},{"location":"levels/#l1-single-source-direct-recall","title":"L1: Single Source Direct Recall","text":"<ul> <li>Tests: Basic fact retrieval from a single source</li> <li>Reasoning type: <code>direct_recall</code></li> <li>Example: \"How many total medals does Norway have?\" (answer is directly stated in one article)</li> <li>What it measures: Can the agent store and retrieve individual facts accurately?</li> </ul>"},{"location":"levels/#l2-multi-source-synthesis","title":"L2: Multi-Source Synthesis","text":"<ul> <li>Tests: Combining information across multiple independent sources</li> <li>Reasoning type: <code>cross_source_synthesis</code>, <code>specific_source_attribution</code></li> <li>Example: \"How many total goals were scored across all matches?\" (requires adding numbers from multiple articles)</li> <li>What it measures: Can the agent integrate information from different sources?</li> </ul>"},{"location":"levels/#l3-temporal-reasoning","title":"L3: Temporal Reasoning","text":"<ul> <li>Tests: Understanding changes over time, computing differences</li> <li>Reasoning type: <code>temporal_comparison</code>, <code>temporal_computation</code></li> <li>Requires: <code>temporal_ordering = True</code></li> <li>Example: \"How much did the stock price change between the two reports?\" (requires computing a delta)</li> <li>What it measures: Can the agent track temporal sequences and reason about changes?</li> </ul>"},{"location":"levels/#l4-procedural-learning","title":"L4: Procedural Learning","text":"<ul> <li>Tests: Learning and applying step-by-step procedures</li> <li>Reasoning type: <code>procedure_recall</code>, <code>conditional_procedure</code></li> <li>Example: \"What are the steps for the basic sourdough method?\" (requires recalling a sequence)</li> <li>What it measures: Can the agent learn and reproduce procedures?</li> </ul>"},{"location":"levels/#l5-contradiction-handling","title":"L5: Contradiction Handling","text":"<ul> <li>Tests: Detecting and reasoning about conflicting information</li> <li>Reasoning type: <code>contradiction_identification</code>, <code>contradiction_resolution</code></li> <li>Example: \"What are the conflicting claims about the vaccine efficacy?\" (two sources disagree)</li> <li>What it measures: Can the agent detect contradictions and reason about them?</li> </ul>"},{"location":"levels/#l6-incremental-learning","title":"L6: Incremental Learning","text":"<ul> <li>Tests: Updating knowledge when new information arrives</li> <li>Reasoning type: <code>knowledge_update</code>, <code>history_awareness</code></li> <li>Requires: <code>update_handling = True</code></li> <li>Example: \"What is the current CEO after the leadership change?\" (requires updating a stored fact)</li> <li>What it measures: Can the agent update beliefs when new information supersedes old?</li> </ul>"},{"location":"levels/#teacher-student-l7","title":"Teacher-Student (L7)","text":""},{"location":"levels/#l7-teaching-session","title":"L7: Teaching Session","text":"<ul> <li>Tests: Agent learns material, then teaches it; graded on teaching accuracy</li> <li>Reasoning type: <code>teaching_accuracy</code>, <code>self_explanation</code></li> <li>Example: Agent learns about photosynthesis, then must explain it clearly enough for a student to understand</li> <li>What it measures: Depth of understanding (the best test of knowledge is teaching it)</li> </ul>"},{"location":"levels/#advanced-levels-l8-l10","title":"Advanced Levels (L8-L10)","text":"<p>These test metacognitive and causal reasoning.</p>"},{"location":"levels/#l8-confidence-calibration","title":"L8: Confidence Calibration","text":"<ul> <li>Tests: Knowing what you know vs. what you do not know</li> <li>Reasoning type: <code>confidence_calibration</code>, <code>uncertainty_detection</code></li> <li>Example: \"How confident are you about X?\" (agent should be uncertain when info is ambiguous)</li> <li>What it measures: Is the agent's confidence well-calibrated to its actual accuracy?</li> </ul>"},{"location":"levels/#l9-causal-reasoning","title":"L9: Causal Reasoning","text":"<ul> <li>Tests: Identifying causal chains and root causes</li> <li>Reasoning type: <code>causal_chain</code>, <code>root_cause</code></li> <li>Example: \"What caused the server outage?\" (requires tracing a chain of events)</li> <li>What it measures: Can the agent identify cause-and-effect relationships?</li> </ul>"},{"location":"levels/#l10-counterfactual-reasoning","title":"L10: Counterfactual Reasoning","text":"<ul> <li>Tests: \"What if X didn't happen?\" reasoning</li> <li>Reasoning type: <code>counterfactual</code>, <code>alternative_outcome</code></li> <li>Example: \"What would have happened if the backup system had worked?\" (requires hypothetical reasoning)</li> <li>What it measures: Can the agent reason about alternative scenarios?</li> </ul>"},{"location":"levels/#novel-skills-l11-l12","title":"Novel Skills (L11-L12)","text":"<p>These test the agent's ability to learn genuinely new capabilities.</p>"},{"location":"levels/#l11-novel-skill-acquisition","title":"L11: Novel Skill Acquisition","text":"<ul> <li>Tests: Learning genuinely new skills from documentation</li> <li>Reasoning type: <code>skill_application</code>, <code>novel_syntax</code></li> <li>Example: Learn a made-up programming syntax, then write code in it</li> <li>What it measures: Can the agent learn and apply truly novel skills?</li> </ul>"},{"location":"levels/#l12-far-transfer","title":"L12: Far Transfer","text":"<ul> <li>Tests: Applying learned reasoning patterns to new domains</li> <li>Reasoning type: <code>cross_domain_transfer</code>, <code>analogical_reasoning</code></li> <li>Example: Learn a pattern in domain A, apply it to solve a problem in domain B</li> <li>What it measures: Can the agent abstract and transfer knowledge across domains?</li> </ul>"},{"location":"levels/#data-structure","title":"Data Structure","text":"<p>Each level is defined as a <code>TestLevel</code> dataclass:</p> <pre><code>@dataclass\nclass TestLevel:\n    level_id: str                       # \"L1\", \"L2\", etc.\n    level_name: str                     # Human-readable name\n    description: str                    # What the level tests\n    articles: list[TestArticle]         # Source content\n    questions: list[TestQuestion]       # Evaluation questions\n    requires_temporal_ordering: bool    # Does order matter?\n    requires_update_handling: bool      # Does info get superseded?\n</code></pre>"},{"location":"levels/#accessing-levels-programmatically","title":"Accessing Levels Programmatically","text":"<pre><code>from amplihack_eval.levels import ALL_LEVELS, get_level_by_id\n\n# Get all core levels (L1-L6)\nfor level in ALL_LEVELS:\n    print(f\"{level.level_id}: {level.level_name} ({len(level.questions)} questions)\")\n\n# Get a specific level\nl3 = get_level_by_id(\"L3\")\nprint(f\"L3 has {len(l3.articles)} articles and {len(l3.questions)} questions\")\n\n# Access level groups\nfrom amplihack_eval.levels import (\n    TEACHER_STUDENT_LEVELS,  # [L7]\n    ADVANCED_LEVELS,         # [L8, L9, L10]\n    NOVEL_SKILL_LEVELS,      # [L11]\n    TRANSFER_LEVELS,         # [L12]\n)\n</code></pre>"},{"location":"levels/#adding-new-levels","title":"Adding New Levels","text":"<p>To add a new evaluation level (e.g., L13):</p> <ol> <li>Define the level in <code>src/amplihack_eval/data/progressive_levels.py</code>:</li> </ol> <pre><code>LEVEL_13 = TestLevel(\n    level_id=\"L13\",\n    level_name=\"Your Level Name\",\n    description=\"What this level tests\",\n    articles=[\n        TestArticle(\n            title=\"Source Article\",\n            content=\"Article content with facts...\",\n            url=\"https://example.com/article\",\n            published=\"2026-01-01T00:00:00Z\",\n        )\n    ],\n    questions=[\n        TestQuestion(\n            question=\"A question about the content\",\n            expected_answer=\"The expected answer\",\n            level=\"L13\",\n            reasoning_type=\"your_reasoning_type\",\n        )\n    ],\n)\n</code></pre> <ol> <li>Add it to the appropriate level group list</li> <li>Re-export it from <code>src/amplihack_eval/levels/__init__.py</code></li> <li>Add tests in <code>tests/test_data_generation.py</code></li> </ol>"},{"location":"multi-agent-eval/","title":"Multi-Agent Evaluation","text":""},{"location":"multi-agent-eval/#status","title":"Status","text":"<p>The multi-agent evaluation module (<code>amplihack_eval.multi_agent_eval</code>) is reserved for future development. The module exists as a placeholder with plans for the following capabilities.</p>"},{"location":"multi-agent-eval/#planned-architecture","title":"Planned Architecture","text":"<p>Multi-agent evaluation will test scenarios where multiple agents collaborate or compete to accomplish tasks. Unlike single-agent evaluation (which tests memory and reasoning), multi-agent evaluation tests coordination, communication, and role specialization.</p>"},{"location":"multi-agent-eval/#planned-scenarios","title":"Planned Scenarios","text":""},{"location":"multi-agent-eval/#collaborative-knowledge-building","title":"Collaborative Knowledge Building","text":"<p>Multiple agents each learn different subsets of information, then must combine their knowledge to answer questions that no single agent could answer alone.</p> <pre><code>Agent A learns: Articles 1-5\nAgent B learns: Articles 6-10\nAgent C learns: Articles 11-15\n\nQuestion: \"Compare findings from Article 3 and Article 12\"\n-&gt; Requires A and C to collaborate\n</code></pre>"},{"location":"multi-agent-eval/#debate-and-consensus","title":"Debate and Consensus","text":"<p>Agents are given ambiguous or contradictory information and must debate to reach a consensus answer. Tests argumentation quality, evidence weighing, and convergence.</p>"},{"location":"multi-agent-eval/#task-delegation","title":"Task Delegation","text":"<p>A coordinator agent receives a complex task and must delegate subtasks to specialist agents, then synthesize their results. Tests planning, delegation, and integration.</p>"},{"location":"multi-agent-eval/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>One agent attempts to inject misleading information while others must maintain accuracy. Tests resilience to adversarial inputs.</p>"},{"location":"multi-agent-eval/#planned-interface","title":"Planned Interface","text":"<p>The multi-agent adapter interface will extend <code>AgentAdapter</code>:</p> <pre><code>class MultiAgentAdapter(AgentAdapter):\n    \"\"\"Adapter for a group of agents that can communicate.\"\"\"\n\n    @abstractmethod\n    def send_message(self, from_agent: str, to_agent: str, message: str) -&gt; str:\n        \"\"\"Send a message between agents.\"\"\"\n\n    @abstractmethod\n    def get_agents(self) -&gt; list[str]:\n        \"\"\"List all agent identifiers in the group.\"\"\"\n\n    @abstractmethod\n    def assign_role(self, agent_id: str, role: str) -&gt; None:\n        \"\"\"Assign a role to a specific agent.\"\"\"\n</code></pre>"},{"location":"multi-agent-eval/#contributing","title":"Contributing","text":"<p>If you are interested in contributing to the multi-agent evaluation module, please open an issue on GitHub to discuss your proposed scenario before implementing.</p>"},{"location":"self-improvement/","title":"Self-Improvement Loop","text":""},{"location":"self-improvement/#overview","title":"Overview","text":"<p>The self-improvement loop automates the process of identifying evaluation weaknesses and iteratively improving agent performance. It follows a disciplined cycle with safety gates at every step.</p>"},{"location":"self-improvement/#the-improvement-cycle","title":"The Improvement Cycle","text":"<pre><code>EVAL -&gt; ANALYZE -&gt; PROPOSE -&gt; CHALLENGE -&gt; VOTE -&gt; APPLY -&gt; RE-EVAL -&gt; DECIDE\n  |                                                                      |\n  +----------------------------------------------------------------------+\n                           (iterate N times)\n</code></pre>"},{"location":"self-improvement/#step-by-step","title":"Step-by-Step","text":"<ol> <li>EVAL: Run the full long-horizon evaluation to get per-category scores</li> <li>ANALYZE: Identify the worst-performing category and diagnose the bottleneck</li> <li>PROPOSE: Use LLM to generate a patch hypothesis targeting the bottleneck</li> <li>CHALLENGE: A devil's advocate reviews the proposal and raises concerns</li> <li>VOTE: 3 independent reviewers vote to accept or reject the proposal</li> <li>APPLY: If majority votes accept, apply the patch (git commit)</li> <li>RE-EVAL: Run evaluation again to measure the patch's impact</li> <li>DECIDE: If regression exceeds threshold, auto-revert; if improvement, keep</li> </ol>"},{"location":"self-improvement/#configuration","title":"Configuration","text":"<pre><code>from amplihack_eval.self_improve.runner import SelfImproveConfig, SelfImproveRunner\n\nconfig = SelfImproveConfig(\n    num_turns=100,              # Dialogue turns per eval\n    num_questions=20,           # Questions per eval\n    seed=42,                    # Random seed for reproducibility\n    max_iterations=3,           # Max improvement iterations\n    failure_threshold=0.7,      # Scores below this are \"failures\"\n    regression_threshold=5.0,   # Max regression (pp) before auto-revert\n    output_dir=\"/tmp/self-improve\",\n    grader_model=\"\",            # LLM model for grading (uses default if empty)\n)\n</code></pre>"},{"location":"self-improvement/#key-components","title":"Key Components","text":""},{"location":"self-improvement/#categoryanalysis","title":"CategoryAnalysis","text":"<p>After each eval run, failures are grouped by question category:</p> <pre><code>@dataclass\nclass CategoryAnalysis:\n    category: str                        # e.g., \"needle_in_haystack\"\n    avg_score: float                     # Average score for this category\n    num_questions: int                   # Number of questions\n    failed_questions: list[dict]         # Details of failed questions\n    bottleneck: str                      # Identified system component\n    suggested_fix: str                   # Suggested improvement\n</code></pre>"},{"location":"self-improvement/#patchproposal-patch_proposerpy","title":"PatchProposal (patch_proposer.py)","text":"<p>The LLM-powered patch generator receives: - The worst category's failure analysis - The full patch history (what was tried before) - System component descriptions</p> <p>It produces a structured proposal with: - Target component - Proposed changes - Expected impact - Risk assessment</p>"},{"location":"self-improvement/#reviewer-voting-reviewer_votingpy","title":"Reviewer Voting (reviewer_voting.py)","text":"<p>Three-stage review process:</p> <ol> <li>Challenge: A devil's advocate reviews the proposal and raises concerns about correctness, side effects, and safety</li> <li>Vote: 3 independent reviewers each vote accept/reject with reasoning</li> <li>Decision: Majority rules; at least 2 of 3 must accept</li> </ol>"},{"location":"self-improvement/#safety-constraints","title":"Safety Constraints","text":"<p>The self-improvement loop enforces strict safety rules:</p> <ul> <li>Never modifies the grader: The grading system is the source of truth</li> <li>Never modifies test data: Test levels and questions are fixed</li> <li>Never modifies safety constraints: Safety gates cannot be weakened</li> <li>Auto-revert on regression: If overall score drops by more than <code>regression_threshold</code> percentage points, the patch is automatically reverted</li> <li>Full history: All attempted patches (successful and failed) are logged to prevent repeating failed fixes</li> </ul>"},{"location":"self-improvement/#cli-usage","title":"CLI Usage","text":"<pre><code># Run 5 iterations of self-improvement\namplihack-eval self-improve --iterations 5 --turns 100 --questions 20\n\n# With custom output directory\namplihack-eval self-improve --iterations 3 --output-dir ./improvement-logs\n</code></pre>"},{"location":"self-improvement/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from amplihack_eval.self_improve.runner import SelfImproveConfig, SelfImproveRunner\n\nconfig = SelfImproveConfig(max_iterations=5, num_turns=100)\nrunner = SelfImproveRunner(config)\n\n# You need to provide an agent factory since the agent may be modified between iterations\nresult = runner.run(agent_factory=lambda: MyAgent())\n\nprint(f\"Score progression: {result.score_progression}\")\nprint(f\"Total iterations: {len(result.iterations)}\")\nfor it in result.iterations:\n    print(f\"  Iteration {it.iteration}: reverted={it.reverted}\")\n</code></pre>"},{"location":"self-improvement/#output-structure","title":"Output Structure","text":"<p>Each run produces a JSON log in the output directory:</p> <pre><code>{\n    \"config\": { ... },\n    \"iterations\": [\n        {\n            \"iteration\": 0,\n            \"report\": { \"overall_score\": 0.72, ... },\n            \"category_analyses\": [ ... ],\n            \"patch_proposal\": { ... },\n            \"review_result\": { \"accepted\": true, \"votes\": [...] },\n            \"post_scores\": { \"overall\": 0.78 },\n            \"reverted\": false\n        }\n    ],\n    \"score_progression\": [0.72, 0.78, 0.82],\n    \"total_duration_seconds\": 342.5\n}\n</code></pre>"}]}