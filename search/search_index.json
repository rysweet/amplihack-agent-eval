{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"amplihack-agent-eval","text":"<p>Evaluation framework for goal-seeking AI agents. Tests memory recall, tool use, planning, and reasoning across progressive difficulty levels (L1-L12).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Long-horizon memory stress tests -- Generates 1000+ turn dialogues with embedded facts, then quizzes the agent on details from various points in the conversation</li> <li>Hybrid grading -- Deterministic (rubric keywords) + LLM (semantic judgment) with multi-vote stability</li> <li>Progressive difficulty levels -- L1 (simple recall) through L12 (far transfer reasoning)</li> <li>Agent-agnostic -- Works with any agent through the <code>AgentAdapter</code> interface</li> <li>Self-improvement loop -- Automated EVAL -&gt; ANALYZE -&gt; PROPOSE -&gt; CHALLENGE -&gt; VOTE -&gt; APPLY -&gt; RE-EVAL cycle</li> <li>Multi-seed holdout -- Run across multiple random seeds to measure inter-seed variance</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation (data generation and adapters, no LLM grading)\npip install amplihack-agent-eval\n\n# With Anthropic grading support\npip install amplihack-agent-eval[anthropic]\n\n# Development\npip install amplihack-agent-eval[dev]\n\n# Everything\npip install amplihack-agent-eval[all,dev]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-implement-the-agentadapter-interface","title":"1. Implement the AgentAdapter interface","text":"<pre><code>from amplihack_eval import AgentAdapter, AgentResponse\n\nclass MyMemoryAgent(AgentAdapter):\n    def __init__(self):\n        self.memory = []\n\n    def learn(self, content: str) -&gt; None:\n        self.memory.append(content)\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        relevant = [m for m in self.memory\n                    if any(w in m.lower() for w in question.lower().split())]\n        return AgentResponse(\n            answer=\" \".join(relevant[:3]) if relevant else \"I don't know\"\n        )\n\n    def reset(self) -&gt; None:\n        self.memory.clear()\n\n    def close(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"#2-run-an-evaluation","title":"2. Run an evaluation","text":"<pre><code>from amplihack_eval import EvalRunner\n\nagent = MyMemoryAgent()\nrunner = EvalRunner(num_turns=100, num_questions=20, grader_votes=3)\nreport = runner.run(agent)\n\nprint(f\"Overall score: {report.overall_score:.2%}\")\nfor cb in report.category_breakdown:\n    print(f\"  {cb.category}: {cb.avg_score:.2%}\")\n</code></pre>"},{"location":"#3-cli-usage","title":"3. CLI usage","text":"<pre><code># Run eval against an HTTP agent\namplihack-eval run --turns 100 --questions 20 --adapter http --agent-url http://localhost:8000\n\n# Run eval with amplihack's LearningAgent\namplihack-eval run --turns 100 --questions 20 --adapter learning-agent\n\n# Multi-seed comparison\namplihack-eval compare --seeds 42,123,456,789 --turns 100\n\n# Self-improvement loop\namplihack-eval self-improve --iterations 5 --turns 100\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"Guide Description Architecture Package layout, core concepts, and design principles Evaluation Levels Complete guide to all 12 progressive difficulty levels (L1-L12) Writing Adapters How to write custom <code>AgentAdapter</code> implementations Self-Improvement Loop Automated improvement cycle with safety gates Multi-Agent Eval Planned multi-agent evaluation scenarios"},{"location":"#environment-variables","title":"Environment Variables","text":"Variable Purpose Default <code>ANTHROPIC_API_KEY</code> Required for LLM grading -- <code>GRADER_MODEL</code> Model for grading <code>claude-sonnet-4-5-20250929</code> <code>EVAL_MODEL</code> Model for LearningAgent adapter <code>claude-sonnet-4-5-20250929</code>"},{"location":"#contributing","title":"Contributing","text":"<pre><code># Clone the repository\ngit clone https://github.com/rysweet/amplihack-agent-eval.git\ncd amplihack-agent-eval\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest tests/ -q\n\n# Run linting\nruff check src/ tests/\nruff format --check src/ tests/\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"API_REFERENCE/","title":"API Reference","text":"<p>Complete reference for all public classes and functions in <code>amplihack-agent-eval</code>.</p>"},{"location":"API_REFERENCE/#top-level-exports-amplihack_eval","title":"Top-Level Exports (<code>amplihack_eval</code>)","text":"<pre><code>from amplihack_eval import (\n    AgentAdapter,       # Abstract base class for agents\n    AgentResponse,      # Response wrapper with tool calls and metadata\n    ToolCall,           # Single tool call record\n    EvalRunner,         # Long-horizon memory evaluation runner\n    EvalResult,         # Per-question evaluation result\n    EvalReport,         # Aggregate evaluation report\n    CategoryBreakdown,  # Per-category score summary\n    DimensionScore,     # Score on a single grading dimension\n    LevelResult,        # Result of a single YAML level\n    SuiteResult,        # Result of multiple YAML levels\n    GradeResult,        # Standalone grading result\n    grade_answer,       # Standalone grading function\n    run_level,          # Run a single YAML level\n    run_suite,          # Run a suite of YAML levels\n)\n</code></pre>"},{"location":"API_REFERENCE/#adapters-amplihack_evaladapters","title":"Adapters (<code>amplihack_eval.adapters</code>)","text":""},{"location":"API_REFERENCE/#agentadapter-abstract-base-class","title":"<code>AgentAdapter</code> (abstract base class)","text":"<p>The interface that all agents must implement to be evaluable.</p> <pre><code>from amplihack_eval.adapters.base import AgentAdapter\n\nclass AgentAdapter(ABC):\n    def learn(self, content: str) -&gt; None: ...\n    def answer(self, question: str) -&gt; AgentResponse: ...\n    def reset(self) -&gt; None: ...\n    def close(self) -&gt; None: ...\n\n    @property\n    def capabilities(self) -&gt; set[str]: ...   # Default: {\"memory\"}\n    @property\n    def name(self) -&gt; str: ...                # Default: class name\n</code></pre> <p>Methods:</p> Method Description <code>learn(content: str)</code> Feed content for memorization. Called once per dialogue turn. <code>answer(question: str) -&gt; AgentResponse</code> Ask a question and return a structured response. <code>reset()</code> Reset state between evaluation runs. <code>close()</code> Clean up resources (connections, temp files). <p>Properties:</p> Property Type Default Description <code>capabilities</code> <code>set[str]</code> <code>{\"memory\"}</code> Declared capabilities. Possible: <code>\"memory\"</code>, <code>\"tool_use\"</code>, <code>\"planning\"</code>. <code>name</code> <code>str</code> Class name Human-readable identifier."},{"location":"API_REFERENCE/#agentresponse","title":"<code>AgentResponse</code>","text":"<p>Wraps an agent's response with optional tool calls, reasoning, and metadata.</p> <pre><code>@dataclass\nclass AgentResponse:\n    answer: str                            # The answer text\n    tool_calls: list[ToolCall] = []        # Tool call trajectory\n    reasoning_trace: str = \"\"              # Chain-of-thought log\n    confidence: float = 0.0                # Self-reported confidence (0.0-1.0)\n    metadata: dict[str, Any] = {}          # Arbitrary key-value pairs\n</code></pre>"},{"location":"API_REFERENCE/#toolcall","title":"<code>ToolCall</code>","text":"<p>Records a single tool invocation within an agent's trajectory.</p> <pre><code>@dataclass\nclass ToolCall:\n    tool_name: str                 # Name of the tool called\n    arguments: dict[str, Any]      # Arguments passed to the tool\n    result: str                    # Tool return value\n    timestamp: float = 0.0        # Unix timestamp of the call\n</code></pre>"},{"location":"API_REFERENCE/#httpadapter","title":"<code>HttpAdapter</code>","text":"<p>Adapter for agents with REST API endpoints.</p> <pre><code>from amplihack_eval.adapters.http_adapter import HttpAdapter\n\nadapter = HttpAdapter(\n    base_url: str,                         # Base URL (trailing slash stripped)\n    learn_endpoint: str = \"/learn\",        # POST endpoint for learning\n    answer_endpoint: str = \"/answer\",      # POST endpoint for answering\n    reset_endpoint: str = \"/reset\",        # POST endpoint for reset\n    timeout: float = 30.0,                 # Request timeout in seconds\n)\n</code></pre> <p>Endpoint contracts: - <code>POST /learn</code> -- Body: <code>{\"content\": \"...\"}</code> -- Response: any (ignored) - <code>POST /answer</code> -- Body: <code>{\"question\": \"...\"}</code> -- Response: <code>{\"answer\": \"...\", \"confidence\": 0.8, ...}</code> - <code>POST /reset</code> -- Body: empty -- Response: any (ignored)</p>"},{"location":"API_REFERENCE/#subprocessadapter","title":"<code>SubprocessAdapter</code>","text":"<p>Adapter for CLI-based agents invoked via subprocess.</p> <pre><code>from amplihack_eval.adapters.subprocess_adapter import SubprocessAdapter\n\nadapter = SubprocessAdapter(\n    command: list[str],                    # Base command (e.g., [\"python\", \"agent.py\"])\n    learn_flag: str = \"--learn\",           # Flag for learn mode\n    answer_flag: str = \"--answer\",         # Flag for answer mode\n    timeout: float = 30.0,                 # Per-call timeout in seconds\n)\n</code></pre> <p>Protocol: Content/question is passed via stdin. Response is captured from stdout.</p>"},{"location":"API_REFERENCE/#learningagentadapter","title":"<code>LearningAgentAdapter</code>","text":"<p>Adapter for amplihack's built-in LearningAgent.</p> <pre><code>from amplihack_eval.adapters.learning_agent import LearningAgentAdapter\n\nadapter = LearningAgentAdapter(\n    model: str = \"\",                       # LLM model (uses EVAL_MODEL env if empty)\n    memory_backend: str = \"sqlite\",        # Memory backend type\n)\n</code></pre>"},{"location":"API_REFERENCE/#core-amplihack_evalcore","title":"Core (<code>amplihack_eval.core</code>)","text":""},{"location":"API_REFERENCE/#evalrunner","title":"<code>EvalRunner</code>","text":"<p>Main evaluation runner. Generates dialogue, feeds to agent, quizzes, grades.</p> <pre><code>runner = EvalRunner(\n    num_turns: int = 1000,         # Number of dialogue turns\n    num_questions: int = 100,      # Number of quiz questions\n    seed: int = 42,                # Random seed\n    grader_votes: int = 3,         # Number of grading votes per question\n)\n</code></pre> <p>Methods:</p> Method Returns Description <code>generate()</code> <code>(GroundTruth, list[Question])</code> Generate dialogue and questions <code>run_dialogue(agent, ground_truth=None)</code> <code>float</code> Feed turns to agent, return learning time (seconds) <code>run(agent, grader_model=\"\")</code> <code>EvalReport</code> Complete pipeline: generate + learn + quiz + grade"},{"location":"API_REFERENCE/#evalresult","title":"<code>EvalResult</code>","text":"<p>Result for a single question.</p> <pre><code>@dataclass\nclass EvalResult:\n    question_id: str                   # Unique question identifier\n    question_text: str                 # The question asked\n    category: str                      # Question category\n    expected_answer: str               # Ground truth answer\n    actual_answer: str                 # Agent's answer\n    dimensions: list[DimensionScore]   # Per-dimension scores\n    overall_score: float               # Average of dimension scores (0.0-1.0)\n    grading_time_s: float = 0.0        # Time spent grading this question\n</code></pre>"},{"location":"API_REFERENCE/#dimensionscore","title":"<code>DimensionScore</code>","text":"<pre><code>@dataclass\nclass DimensionScore:\n    dimension: str         # Dimension name (e.g., \"factual_accuracy\")\n    score: float           # 0.0 to 1.0\n    reasoning: str = \"\"    # Grader's reasoning for the score\n</code></pre>"},{"location":"API_REFERENCE/#categorybreakdown","title":"<code>CategoryBreakdown</code>","text":"<pre><code>@dataclass\nclass CategoryBreakdown:\n    category: str                              # Category name\n    num_questions: int                         # Number of questions\n    avg_score: float                           # Average score\n    min_score: float                           # Minimum score\n    max_score: float                           # Maximum score\n    dimension_averages: dict[str, float] = {}  # Per-dimension averages\n</code></pre>"},{"location":"API_REFERENCE/#evalreport","title":"<code>EvalReport</code>","text":"<pre><code>@dataclass\nclass EvalReport:\n    num_turns: int\n    num_questions: int\n    total_facts_delivered: int\n    learning_time_s: float\n    questioning_time_s: float\n    grading_time_s: float\n    overall_score: float                           # 0.0-1.0\n    category_breakdown: list[CategoryBreakdown]\n    results: list[EvalResult]\n    memory_stats: dict[str, Any] = {}\n</code></pre> <p>Methods: <code>to_dict() -&gt; dict[str, Any]</code> -- JSON-serializable dictionary.</p>"},{"location":"API_REFERENCE/#levelresult","title":"<code>LevelResult</code>","text":"<pre><code>@dataclass\nclass LevelResult:\n    level_id: str\n    level_name: str\n    passed: bool\n    overall_score: float\n    pass_threshold: float\n    results: list[EvalResult]\n    category_breakdown: list[CategoryBreakdown]\n</code></pre>"},{"location":"API_REFERENCE/#suiteresult","title":"<code>SuiteResult</code>","text":"<pre><code>@dataclass\nclass SuiteResult:\n    level_results: list[LevelResult]\n    overall_score: float\n    passed_count: int\n    total_count: int\n    skipped: list[str]     # Levels skipped due to failed prerequisites\n</code></pre>"},{"location":"API_REFERENCE/#run_level","title":"<code>run_level()</code>","text":"<pre><code>def run_level(\n    level_id: str,             # e.g., \"L01\"\n    agent: AgentAdapter,\n    grader_model: str = \"\",\n) -&gt; LevelResult\n</code></pre>"},{"location":"API_REFERENCE/#run_suite","title":"<code>run_suite()</code>","text":"<pre><code>def run_suite(\n    level_ids: list[str],\n    agent: AgentAdapter,\n    grader_model: str = \"\",\n) -&gt; SuiteResult\n</code></pre>"},{"location":"API_REFERENCE/#grade_answer","title":"<code>grade_answer()</code>","text":"<pre><code>def grade_answer(\n    actual_answer: str,\n    expected_answer: str,\n    level: str = \"\",\n    grading_model: str = \"\",\n) -&gt; GradeResult\n</code></pre>"},{"location":"API_REFERENCE/#data-generation-amplihack_evaldata","title":"Data Generation (<code>amplihack_eval.data</code>)","text":""},{"location":"API_REFERENCE/#generate_dialogue","title":"<code>generate_dialogue()</code>","text":"<pre><code>def generate_dialogue(num_turns: int = 1000, seed: int = 42) -&gt; GroundTruth\n</code></pre>"},{"location":"API_REFERENCE/#generate_questions","title":"<code>generate_questions()</code>","text":"<pre><code>def generate_questions(ground_truth: GroundTruth, num_questions: int = 100) -&gt; list[Question]\n</code></pre>"},{"location":"API_REFERENCE/#key-dataclasses","title":"Key Dataclasses","text":"<p><code>GroundTruth</code>: <code>turns</code>, <code>facts_by_entity</code>, <code>current_values</code>, <code>superseded_values</code></p> <p><code>Turn</code>: <code>turn_number</code>, <code>content</code>, <code>block</code>, <code>block_name</code>, <code>facts</code></p> <p><code>Question</code>: <code>question_id</code>, <code>text</code>, <code>expected_answer</code>, <code>category</code>, <code>relevant_turns</code>, <code>scoring_dimensions</code>, <code>chain_length</code>, <code>rubric</code></p> <p><code>GradingRubric</code>: <code>required_keywords</code>, <code>acceptable_paraphrases</code>, <code>incorrect_patterns</code>, <code>dimension_weights</code></p>"},{"location":"API_REFERENCE/#progressive-levels","title":"Progressive Levels","text":"<pre><code>from amplihack_eval.data.progressive_levels import (\n    ALL_LEVELS,             # L1-L6\n    TEACHER_STUDENT_LEVELS, # [L7]\n    ADVANCED_LEVELS,        # [L8, L9, L10]\n    NOVEL_SKILL_LEVELS,     # [L11]\n    TRANSFER_LEVELS,        # [L12]\n    get_level_by_id,        # (str) -&gt; TestLevel | None\n)\n</code></pre>"},{"location":"API_REFERENCE/#extended-scenarios","title":"Extended Scenarios","text":"<p>L13: <code>amplihack_eval.data.tool_use_scenarios</code> -- <code>ToolDefinition</code>, <code>ToolUseScenario</code>, <code>ALL_TOOL_USE_SCENARIOS</code></p> <p>L14: <code>amplihack_eval.data.forgetting_scenarios</code> -- <code>FactUpdate</code>, <code>ForgettingScenario</code>, <code>ALL_FORGETTING_SCENARIOS</code></p> <p>L15: <code>amplihack_eval.data.adversarial_scenarios</code> -- <code>KnowledgeBaseFact</code>, <code>AdversarialScenario</code>, <code>ALL_ADVERSARIAL_SCENARIOS</code></p> <p>L16: <code>amplihack_eval.data.decision_scenarios</code> -- <code>ContextFact</code>, <code>DecisionScenario</code>, <code>ALL_DECISION_SCENARIOS</code></p>"},{"location":"API_REFERENCE/#levels-amplihack_evallevels","title":"Levels (<code>amplihack_eval.levels</code>)","text":""},{"location":"API_REFERENCE/#yaml-schema","title":"YAML Schema","text":"<p><code>LevelDefinition</code>: <code>id</code>, <code>name</code>, <code>description</code>, <code>category</code>, <code>difficulty</code>, <code>questions</code>, <code>scoring</code>, <code>prerequisites</code>, <code>data_source</code>, <code>min_turns</code>, <code>grading_mode</code></p> <p><code>ScoringConfig</code>: <code>pass_threshold</code>, <code>dimensions</code>, <code>weights</code>, <code>grader_votes</code></p> <p><code>QuestionTemplate</code>: <code>id</code>, <code>text</code>, <code>category</code>, <code>scoring_dimensions</code>, <code>expected_answer</code>, <code>rubric</code></p>"},{"location":"API_REFERENCE/#loader","title":"Loader","text":"<pre><code>from amplihack_eval.levels.loader import load_level, load_all_levels, validate_level\n</code></pre>"},{"location":"API_REFERENCE/#level-specific-scorers","title":"Level-Specific Scorers","text":"<ul> <li><code>L13_tool_selection</code>: <code>ToolTrajectory</code>, <code>ToolSelectionScore</code></li> <li><code>L14_selective_forgetting</code>: <code>ForgettingResult</code></li> <li><code>L15_adversarial_recall</code>: <code>AdversarialRecallScore</code></li> <li><code>L16_decision_from_memory</code>: <code>DecisionScore</code></li> </ul>"},{"location":"API_REFERENCE/#self-improvement-amplihack_evalself_improve","title":"Self-Improvement (<code>amplihack_eval.self_improve</code>)","text":""},{"location":"API_REFERENCE/#run_self_improve","title":"<code>run_self_improve()</code>","text":"<pre><code>def run_self_improve(\n    config: SelfImproveConfig,\n    agent_factory: Callable[[], AgentAdapter],\n    llm_call: Callable[[str], str] | None = None,\n    project_root: Path | None = None,\n) -&gt; RunnerResult\n</code></pre>"},{"location":"API_REFERENCE/#key-classes","title":"Key Classes","text":"<p><code>SelfImproveConfig</code>: <code>num_turns</code>, <code>num_questions</code>, <code>seed</code>, <code>max_iterations</code>, <code>failure_threshold</code>, <code>regression_threshold</code>, <code>output_dir</code>, <code>grader_model</code></p> <p><code>RunnerResult</code>: <code>config</code>, <code>iterations</code>, <code>score_progression</code>, <code>category_progression</code>, <code>total_duration_seconds</code></p> <p><code>IterationResult</code>: <code>iteration</code>, <code>report</code>, <code>category_analyses</code>, <code>improvements_applied</code>, <code>patch_proposal</code>, <code>review_result</code>, <code>post_scores</code>, <code>reverted</code>, <code>revert_reason</code></p> <p><code>CategoryAnalysis</code>: <code>category</code>, <code>avg_score</code>, <code>num_questions</code>, <code>failed_questions</code>, <code>bottleneck</code>, <code>suggested_fix</code></p>"},{"location":"API_REFERENCE/#patch-proposer","title":"Patch Proposer","text":"<p><code>PatchProposal</code>: <code>target_file</code>, <code>hypothesis</code>, <code>description</code>, <code>diff</code>, <code>expected_impact</code>, <code>risk_assessment</code>, <code>confidence</code></p> <p><code>PatchHistory</code>: <code>applied_patches</code>, <code>reverted_patches</code>, <code>rejected_patches</code></p>"},{"location":"API_REFERENCE/#reviewer-voting","title":"Reviewer Voting","text":"<p><code>ReviewVote</code>: <code>reviewer_id</code>, <code>vote</code>, <code>rationale</code>, <code>concerns</code>, <code>suggested_modifications</code></p> <p><code>ChallengeResponse</code>: <code>challenge_arguments</code>, <code>proposer_response</code>, <code>concerns_addressed</code>, <code>remaining_concerns</code></p> <p><code>ReviewResult</code>: <code>proposal</code>, <code>challenge</code>, <code>votes</code>, <code>decision</code>, <code>consensus_rationale</code></p>"},{"location":"API_REFERENCE/#multi-agent-evaluation-amplihack_evalmulti_agent_eval","title":"Multi-Agent Evaluation (<code>amplihack_eval.multi_agent_eval</code>)","text":""},{"location":"API_REFERENCE/#evalcoordinator","title":"<code>EvalCoordinator</code>","text":"<pre><code>coordinator = EvalCoordinator(grader_agents=3, enable_adversary=True)\nreport = coordinator.run_eval(agent, EvalConfig(num_turns=100))\n</code></pre>"},{"location":"API_REFERENCE/#multiagentevalpipeline","title":"<code>MultiAgentEvalPipeline</code>","text":"<pre><code>pipeline = MultiAgentEvalPipeline()\nreport = pipeline.run(PipelineConfig(adversarial_rounds=2))\n</code></pre>"},{"location":"API_REFERENCE/#agent-types","title":"Agent Types","text":"<ul> <li><code>GraderAgent</code> -- perspectives: <code>\"factual\"</code>, <code>\"reasoning\"</code>, <code>\"completeness\"</code></li> <li><code>AdversaryAgent</code> -- generates targeted hard questions</li> <li><code>AnalystAgent</code> -- produces <code>AnalysisReport</code>, <code>ComparisonReport</code>, <code>Improvement</code></li> </ul>"},{"location":"CATEGORIES/","title":"Evaluation Categories","text":"<p>This document provides a detailed explanation of every evaluation category in <code>amplihack-agent-eval</code>, from the progressive levels (L1--L16) to the long-horizon question categories.</p>"},{"location":"CATEGORIES/#progressive-levels-l1-l12","title":"Progressive Levels (L1--L12)","text":"<p>These are hand-crafted evaluation levels with curated articles and questions. Each level isolates a specific cognitive capability.</p>"},{"location":"CATEGORIES/#l1-single-source-direct-recall","title":"L1: Single Source Direct Recall","text":"<p>Difficulty: 1/5 | Category: memory | Prerequisites: none</p> <p>What it tests: The most basic memory operation -- retrieving a fact that was directly stated in a single source. No inference, no synthesis, no temporal reasoning required.</p> <p>Example articles: Medal standings from a sporting event with specific numbers.</p> <p>Example questions: - \"How many total medals does Norway have as of February 15?\" (Expected: \"26 total medals\") - \"Which country is in second place?\" (Expected: \"Italy with 22 total medals\")</p> <p>Reasoning types: <code>direct_recall</code></p> <p>Why it matters: This is the baseline. If an agent fails L1, its memory system has fundamental issues with storage or retrieval. Every higher level depends on this capability.</p>"},{"location":"CATEGORIES/#l2-multi-source-synthesis","title":"L2: Multi-Source Synthesis","text":"<p>Difficulty: 2/5 | Category: memory | Prerequisites: L1</p> <p>What it tests: Combining information from 2--3 independent sources to answer a question that no single source can answer alone.</p> <p>Example articles: Medal standings + individual athlete achievements + historical context (3 separate articles).</p> <p>Example questions: - \"How does Italy's 2026 gold medal performance compare to their previous best?\" (requires standings article + historical article) - \"Which country's individual athletes won the most medals mentioned in the athlete achievements article?\" (requires counting across sources)</p> <p>Reasoning types: <code>cross_source_synthesis</code></p> <p>Why it matters: Real-world information rarely comes from a single source. Agents must integrate across sources to provide complete answers.</p>"},{"location":"CATEGORIES/#l3-temporal-reasoning","title":"L3: Temporal Reasoning","text":"<p>Difficulty: 3/5 | Category: memory | Prerequisites: L1</p> <p>What it tests: Tracking values that change over time and computing differences between temporal snapshots.</p> <p>Example articles: Medal standings on Day 7, Day 9, and Day 10 (three timestamped updates).</p> <p>Example questions: - \"How many medals did Norway win between Day 7 and Day 9?\" (Expected: \"8 medals (from 18 to 26)\") - \"Describe the trend in Italy's gold medal performance over the three days\" (Expected: \"+3 golds Day 7-9, then +1 gold Day 9-10, slowing\")</p> <p>Reasoning types: <code>temporal_difference</code>, <code>temporal_comparison</code>, <code>temporal_trend</code></p> <p>Requires: <code>temporal_ordering = True</code></p> <p>Why it matters: Agents operating in dynamic environments must understand not just current state but how things have changed.</p>"},{"location":"CATEGORIES/#l4-procedural-learning","title":"L4: Procedural Learning","text":"<p>Difficulty: 2/5 | Category: memory | Prerequisites: L1</p> <p>What it tests: Learning a multi-step procedure from documentation and reproducing or applying it.</p> <p>Example articles: A detailed Flutter development setup guide with 9 numbered steps and troubleshooting tips.</p> <p>Example questions: - \"What command creates a new Flutter project?\" (Expected: \"flutter create my_app\") - \"Describe the complete workflow from creating a project to running tests\" (requires recalling the ordered steps) - \"If I want to create a project called 'weather_app' and add the http package, what exact commands would I run?\" (requires applying the procedure to a new scenario)</p> <p>Reasoning types: <code>procedural_recall</code>, <code>procedural_troubleshooting</code>, <code>procedural_sequence</code>, <code>procedural_application</code></p> <p>Why it matters: Many real tasks involve following procedures. Agents must be able to learn, recall, and adapt procedures to new contexts.</p>"},{"location":"CATEGORIES/#l5-contradiction-handling","title":"L5: Contradiction Handling","text":"<p>Difficulty: 3/5 | Category: memory | Prerequisites: L1, L2</p> <p>What it tests: Detecting when two or more sources provide conflicting information and reasoning about the conflict.</p> <p>Example articles: Two articles about Olympics viewership -- one from the IOC claiming 1.2 billion viewers, another from independent analysts claiming 800 million.</p> <p>Example questions: - \"How many people watched the 2026 opening ceremony?\" (Expected: \"Conflicting reports: IOC estimates 1.2 billion, independent analysts report 800 million\") - \"Which viewership figure would you consider more reliable and why?\" (requires source credibility reasoning)</p> <p>Reasoning types: <code>contradiction_detection</code>, <code>contradiction_reasoning</code>, <code>source_credibility</code></p> <p>Why it matters: In real-world information environments, sources frequently disagree. An agent that picks one value without acknowledging the conflict is unreliable. An agent that acknowledges both values and reasons about the discrepancy demonstrates understanding.</p>"},{"location":"CATEGORIES/#l6-incremental-learning","title":"L6: Incremental Learning","text":"<p>Difficulty: 3/5 | Category: memory | Prerequisites: L1</p> <p>What it tests: Updating knowledge when new information supersedes old information.</p> <p>Example articles: An article from Feb 15 stating an athlete has 9 gold medals, followed by a Feb 17 article updating the count to 10.</p> <p>Example questions: - \"How many Olympic gold medals does Johannes Klaebo have?\" (Expected: \"10\" -- must use the update, not the original) - \"How did Klaebo's record change between February 15 and February 17?\" (Expected: \"Increased from 9 to 10\")</p> <p>Reasoning types: <code>incremental_update</code>, <code>incremental_tracking</code>, <code>incremental_synthesis</code></p> <p>Requires: <code>update_handling = True</code></p> <p>Why it matters: An agent that returns stale data when a newer value exists is dangerous, especially in time-sensitive domains. This level tests the critical ability to supersede outdated information.</p>"},{"location":"CATEGORIES/#l7-teacher-student-knowledge-transfer","title":"L7: Teacher-Student Knowledge Transfer","text":"<p>Difficulty: 3/5 | Category: memory | Prerequisites: L1, L2</p> <p>What it tests: The agent learns material and then must \"teach\" it accurately. This tests depth of understanding -- superficial memorization is not sufficient for accurate teaching.</p> <p>Example: The agent learns multi-source Olympic data, then answers questions as if explaining to someone who has not read the articles.</p> <p>Reasoning types: <code>knowledge_transfer_recall</code>, <code>knowledge_transfer_synthesis</code></p> <p>Why it matters: Based on the educational principle that the best test of knowledge is being able to teach it (Chi, 1994). An agent that can teach accurately has genuinely internalized the material.</p>"},{"location":"CATEGORIES/#l8-metacognition-confidence-calibration","title":"L8: Metacognition / Confidence Calibration","text":"<p>Difficulty: 4/5 | Category: reasoning | Prerequisites: L1</p> <p>What it tests: Whether the agent knows what it knows and what it does not know. Can it calibrate its confidence appropriately?</p> <p>Example questions: - \"How confident should you be in answering 'How many medals does Canada have?'\" (Expected: \"Low confidence -- Canada is not in the data\") - \"Which of these questions can you answer with HIGH confidence and which with LOW?\" (requires distinguishing answerable from unanswerable)</p> <p>Reasoning types: <code>confidence_calibration</code>, <code>gap_identification</code>, <code>confidence_discrimination</code></p> <p>Why it matters: Based on the MUSE framework (arXiv 2024) and Dunlosky &amp; Metcalfe (2009). Overconfident agents are dangerous -- they present guesses as facts. Well-calibrated agents are trustworthy because they say \"I don't know\" when appropriate.</p>"},{"location":"CATEGORIES/#l9-causal-reasoning","title":"L9: Causal Reasoning","text":"<p>Difficulty: 4/5 | Category: reasoning | Prerequisites: L1, L3</p> <p>What it tests: Identifying causal chains and root causes from correlated observations.</p> <p>Example: An article describing Italy's Olympic improvement, listing 5 contributing factors with causal relationships between them.</p> <p>Example questions: - \"What caused Italy to improve from 3 golds in 2018 to 8 golds in 2026?\" (requires tracing the causal chain) - \"Which single factor was most important for Italy's improvement?\" (requires identifying the root cause)</p> <p>Reasoning types: <code>causal_chain</code>, <code>counterfactual_causal</code>, <code>root_cause_analysis</code></p> <p>Why it matters: Based on Pearl's causal hierarchy (2009). Correlation is not causation -- agents must be able to trace cause-and-effect relationships to provide useful analysis.</p>"},{"location":"CATEGORIES/#l10-counterfactual-reasoning","title":"L10: Counterfactual Reasoning","text":"<p>Difficulty: 5/5 | Category: reasoning | Prerequisites: L1, L9</p> <p>What it tests: \"What if X didn't happen?\" reasoning about hypothetical alternatives.</p> <p>Example questions: - \"If Klaebo had not competed, would Norway still have led the gold medal count?\" (requires computing the counterfactual medal standings) - \"In a world where cross-country skiing was removed from the Olympics, how would standings change?\" (requires structural counterfactual)</p> <p>Reasoning types: <code>counterfactual_removal</code>, <code>counterfactual_timing</code>, <code>counterfactual_structural</code></p> <p>Why it matters: Based on Byrne (2005) and the nearest-possible-world constraint. Counterfactual reasoning is essential for planning, risk assessment, and decision making.</p>"},{"location":"CATEGORIES/#l11-novel-skill-acquisition","title":"L11: Novel Skill Acquisition","text":"<p>Difficulty: 5/5 | Category: reasoning | Prerequisites: L1, L4</p> <p>What it tests: Learning a genuinely new skill from documentation and applying it to solve problems. The skill must be post-training-cutoff to ensure the agent is actually learning, not recalling.</p> <p>Example: GitHub Agentic Workflows (gh-aw) documentation, including frontmatter syntax, compilation rules, and security architecture.</p> <p>Example questions: - \"Write a gh-aw workflow file that runs on PR creation and checks for security issues\" (requires applying learned syntax) - \"What happens if you edit the markdown body of a workflow -- do you need to recompile?\" (requires understanding compilation rules)</p> <p>Reasoning types: <code>skill_application</code>, <code>novel_syntax</code>, <code>skill_troubleshooting</code></p> <p>Why it matters: The true test of an agent's learning capability is not recall of training data but the ability to learn genuinely new skills from documentation encountered at runtime.</p>"},{"location":"CATEGORIES/#l12-far-transfer","title":"L12: Far Transfer","text":"<p>Difficulty: 5/5 | Category: reasoning | Prerequisites: L1, L9, L11</p> <p>What it tests: Applying reasoning patterns learned in one domain to solve problems in a completely different domain.</p> <p>Example: Learn Olympic medal analysis patterns, then apply the same analytical framework to a business scenario.</p> <p>Reasoning types: <code>cross_domain_transfer</code>, <code>analogical_reasoning</code></p> <p>Why it matters: Transfer learning is the hallmark of genuine understanding. An agent that can abstract patterns and apply them to new domains demonstrates deep reasoning rather than pattern matching.</p>"},{"location":"CATEGORIES/#extended-levels-l13-l16","title":"Extended Levels (L13--L16)","text":"<p>These levels extend the evaluation beyond memory and reasoning into tool use, information management, robustness, and decision making.</p>"},{"location":"CATEGORIES/#l13-tool-selection","title":"L13: Tool Selection","text":"<p>Difficulty: 3/5 | Category: tool_use</p> <p>What it tests: Given a set of available tools, can the agent select the right ones and chain them in the correct order?</p> <p>Metrics: - <code>tool_selection_accuracy</code> -- Did the agent pick the right tools? (Jaccard similarity) - <code>tool_efficiency</code> -- Were there unnecessary tool calls? (optimal / actual ratio) - <code>tool_chain_correctness</code> -- Was the ordering correct? (longest common subsequence)</p> <p>Scenarios: Multi-domain scenarios (DevOps, data analysis, security) with expected tool sequences.</p> <p>Example: \"Deploy a new version of the API service\" -- expected tools: <code>check_tests</code>, <code>build_container</code>, <code>push_registry</code>, <code>update_deployment</code>, <code>verify_health</code></p> <p>Why it matters: Real agents must select and orchestrate tools. Tool selection errors compound -- picking the wrong tool at step 1 invalidates the entire chain.</p>"},{"location":"CATEGORIES/#l14-selective-forgetting","title":"L14: Selective Forgetting","text":"<p>Difficulty: 3/5 | Category: memory</p> <p>What it tests: When facts are updated, does the agent return the current value and not present the old value as current?</p> <p>Metrics: - <code>current_value_accuracy</code> -- Does the agent return the current value? - <code>stale_data_penalty</code> -- Does it present old values as current? - <code>update_awareness</code> -- Does it acknowledge that the value was updated?</p> <p>Example: Server configuration changes from 16GB RAM to 32GB RAM. The agent should return 32GB and ideally note the upgrade.</p> <p>Why it matters: An agent that remembers everything indiscriminately is dangerous. The ability to deprioritize superseded information is as important as recall. This is distinct from L6 (incremental learning) -- L14 specifically penalizes presenting stale data as current.</p>"},{"location":"CATEGORIES/#l15-adversarial-recall","title":"L15: Adversarial Recall","text":"<p>Difficulty: 4/5 | Category: reasoning</p> <p>What it tests: Resistance to hallucination when asked plausible-but-wrong questions about information not in the knowledge base.</p> <p>Metrics: - <code>hallucination_resistance</code> -- Does the agent say \"I don't know\" when appropriate? - <code>fact_boundary_awareness</code> -- Can it distinguish known from unknown? - <code>confidence_calibration</code> -- Is it appropriately uncertain?</p> <p>Scenarios: Questions that reference entities similar to but different from what the agent has learned (e.g., asking about \"Project Falcon\" when the agent only knows about \"Project Atlas\").</p> <p>Why it matters: Agents that always provide an answer, even when they do not have the information, are unreliable and potentially harmful. Honest uncertainty is a sign of intelligence, not weakness.</p>"},{"location":"CATEGORIES/#l16-decision-from-memory","title":"L16: Decision From Memory","text":"<p>Difficulty: 5/5 | Category: reasoning</p> <p>What it tests: The highest cognitive level -- can the agent recall facts, analyze them, and make correct decisions?</p> <p>Metrics: - <code>decision_quality</code> -- Is the decision correct given the available facts? - <code>reasoning_quality</code> -- Does the explanation reference the correct facts? - <code>fact_usage</code> -- Were the right facts used to support the decision?</p> <p>Scenarios: Decision problems across domains (hiring, infrastructure planning, security response) where the correct decision depends on facts the agent has previously learned.</p> <p>Example: \"Based on the project timelines and team availability you've learned about, which project should receive additional resources?\" (requires recalling multiple project states and reasoning about the best allocation)</p> <p>Why it matters: Memory without application is useless. The ultimate test is whether stored knowledge leads to good decisions.</p>"},{"location":"CATEGORIES/#long-horizon-question-categories","title":"Long-Horizon Question Categories","text":"<p>These categories are generated dynamically from the 12-block dialogue in the long-horizon evaluation. They overlap conceptually with the progressive levels but are tested at much larger scale.</p>"},{"location":"CATEGORIES/#needle_in_haystack","title":"<code>needle_in_haystack</code>","text":"<p>Direct fact retrieval from a specific person, project, or technical domain -- finding one fact among potentially thousands of turns. Tests basic retrieval fidelity.</p>"},{"location":"CATEGORIES/#temporal_evolution","title":"<code>temporal_evolution</code>","text":"<p>Tracking changes to projects (deadlines, budgets, leads) and the evolving story. Questions require knowing not just the current value but the history of changes.</p>"},{"location":"CATEGORIES/#numerical_precision","title":"<code>numerical_precision</code>","text":"<p>Exact recall of specific numbers, percentages, monetary values, and metrics. No approximation accepted.</p>"},{"location":"CATEGORIES/#source_attribution","title":"<code>source_attribution</code>","text":"<p>Identifying which source made a specific claim, especially when multiple sources discuss the same topic with different values.</p>"},{"location":"CATEGORIES/#cross_reference","title":"<code>cross_reference</code>","text":"<p>Connecting facts across blocks -- e.g., linking a person from Block 1 to a project from Block 2 to a technical fact from Block 3.</p>"},{"location":"CATEGORIES/#distractor_resistance","title":"<code>distractor_resistance</code>","text":"<p>Answering questions accurately when irrelevant fun facts from Block 8 might confuse retrieval.</p>"},{"location":"CATEGORIES/#meta_memory","title":"<code>meta_memory</code>","text":"<p>Questions about what the agent knows -- counts, categories, completeness of knowledge.</p>"},{"location":"CATEGORIES/#security_log_analysis","title":"<code>security_log_analysis</code>","text":"<p>Pattern recognition in structured security events -- identifying attack sequences, mapping IP addresses to events, recognizing known attack patterns.</p>"},{"location":"CATEGORIES/#incident_tracking","title":"<code>incident_tracking</code>","text":"<p>Following incident timelines from creation through investigation to resolution.</p>"},{"location":"CATEGORIES/#infrastructure_knowledge","title":"<code>infrastructure_knowledge</code>","text":"<p>Recall of specific infrastructure configuration details.</p>"},{"location":"CATEGORIES/#problem_solving","title":"<code>problem_solving</code>","text":"<p>Applying stored problem-solution knowledge to questions.</p>"},{"location":"CATEGORIES/#multi_hop_reasoning","title":"<code>multi_hop_reasoning</code>","text":"<p>Questions requiring 2+ retrieval steps to compose the answer from multiple facts.</p>"},{"location":"CATEGORIES/#category-to-skill-mapping","title":"Category-to-Skill Mapping","text":"Category Primary Skill Secondary Skill <code>needle_in_haystack</code> Retrieval precision Entity recognition <code>temporal_evolution</code> Temporal ordering Change tracking <code>numerical_precision</code> Exact recall Number handling <code>source_attribution</code> Provenance tracking Source disambiguation <code>cross_reference</code> Graph traversal Entity linking <code>distractor_resistance</code> Relevance filtering Confidence weighting <code>meta_memory</code> Self-awareness Aggregation <code>security_log_analysis</code> Pattern recognition Structured data parsing <code>incident_tracking</code> State machine tracking Timeline reconstruction <code>infrastructure_knowledge</code> Configuration recall Specification matching <code>problem_solving</code> Solution retrieval Analogical reasoning <code>multi_hop_reasoning</code> Compositional recall Fact chaining L13 Tool Selection Planning Tool knowledge L14 Selective Forgetting Update management Stale data detection L15 Adversarial Recall Honesty Boundary awareness L16 Decision From Memory Application Analytical reasoning"},{"location":"EXTENDING/","title":"Extending the Eval Framework","text":"<p>This guide explains how to extend <code>amplihack-agent-eval</code> with custom agent adapters, new question categories, custom data generators, and new grading dimensions.</p>"},{"location":"EXTENDING/#writing-a-custom-agentadapter","title":"Writing a Custom AgentAdapter","text":"<p>The <code>AgentAdapter</code> is the interface between the eval framework and your agent. To evaluate any agent, implement this abstract base class.</p>"},{"location":"EXTENDING/#minimal-implementation","title":"Minimal Implementation","text":"<pre><code>from amplihack_eval.adapters.base import AgentAdapter, AgentResponse\n\nclass MyAgent(AgentAdapter):\n    \"\"\"Adapter for my custom agent.\"\"\"\n\n    def __init__(self, model_name: str = \"my-model\"):\n        self._model = model_name\n        self._memory: list[str] = []\n\n    def learn(self, content: str) -&gt; None:\n        \"\"\"Feed content to the agent for memorization.\n\n        Called once per dialogue turn. The agent should store this content\n        in whatever memory system it uses.\n        \"\"\"\n        self._memory.append(content)\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        \"\"\"Ask the agent a question and return its response.\n\n        Must return an AgentResponse, which wraps the answer text along\n        with optional tool calls, reasoning trace, confidence, and metadata.\n        \"\"\"\n        # Your agent's actual answering logic here\n        answer_text = self._my_query_logic(question)\n        return AgentResponse(\n            answer=answer_text,\n            confidence=0.8,\n            metadata={\"model\": self._model},\n        )\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the agent's state between evaluation runs.\n\n        Called by multi-seed evaluation and self-improvement loop to\n        ensure each run starts from a clean state.\n        \"\"\"\n        self._memory.clear()\n\n    def close(self) -&gt; None:\n        \"\"\"Clean up resources (connections, temp files, etc.).\n\n        Called when evaluation is complete.\n        \"\"\"\n        pass\n\n    def _my_query_logic(self, question: str) -&gt; str:\n        # Implement your actual query logic\n        return \"I don't know\"\n</code></pre>"},{"location":"EXTENDING/#adding-tool-call-tracking","title":"Adding Tool Call Tracking","text":"<p>If your agent uses tools, capture the trajectory in <code>AgentResponse</code>:</p> <pre><code>from amplihack_eval.adapters.base import AgentResponse, ToolCall\n\ndef answer(self, question: str) -&gt; AgentResponse:\n    # Agent does tool calls\n    tool_calls = []\n\n    # Example: agent calls a search tool\n    search_result = self._search(question)\n    tool_calls.append(ToolCall(\n        tool_name=\"search\",\n        arguments={\"query\": question},\n        result=search_result,\n        timestamp=time.time(),\n    ))\n\n    answer_text = self._generate_answer(question, search_result)\n\n    return AgentResponse(\n        answer=answer_text,\n        tool_calls=tool_calls,\n        reasoning_trace=\"Searched memory, found relevant fact, composed answer\",\n        confidence=0.9,\n    )\n</code></pre> <p>The L13 tool selection evaluation uses <code>tool_calls</code> to score tool selection accuracy, efficiency, and chain correctness.</p>"},{"location":"EXTENDING/#adding-custom-capabilities","title":"Adding Custom Capabilities","text":"<p>Override the <code>capabilities</code> property to declare what your agent can do:</p> <pre><code>@property\ndef capabilities(self) -&gt; set[str]:\n    return {\"memory\", \"tool_use\", \"planning\"}\n\n@property\ndef name(self) -&gt; str:\n    return f\"MyAgent({self._model})\"\n</code></pre> <p>Capabilities are used by the evaluation framework to skip levels that require capabilities the agent does not have.</p>"},{"location":"EXTENDING/#using-built-in-adapters","title":"Using Built-In Adapters","text":"<p>Three adapters are included for common integration patterns:</p> <p>HttpAdapter -- for agents with REST API endpoints:</p> <pre><code>from amplihack_eval.adapters.http_adapter import HttpAdapter\n\nadapter = HttpAdapter(\n    base_url=\"http://localhost:8000\",\n    learn_endpoint=\"/learn\",      # POST, body: {\"content\": \"...\"}\n    answer_endpoint=\"/answer\",    # POST, body: {\"question\": \"...\"}\n    reset_endpoint=\"/reset\",      # POST, empty body\n    timeout=30.0,\n)\n</code></pre> <p>SubprocessAdapter -- for CLI-based agents:</p> <pre><code>from amplihack_eval.adapters.subprocess_adapter import SubprocessAdapter\n\nadapter = SubprocessAdapter(\n    command=[\"python\", \"my_agent.py\"],\n    learn_flag=\"--learn\",     # Passes content via stdin after this flag\n    answer_flag=\"--answer\",   # Passes question via stdin after this flag\n    timeout=30.0,\n)\n</code></pre> <p>LearningAgentAdapter -- for amplihack's built-in LearningAgent:</p> <pre><code>from amplihack_eval.adapters.learning_agent import LearningAgentAdapter\n\nadapter = LearningAgentAdapter(model=\"claude-sonnet-4-5-20250929\")\n</code></pre>"},{"location":"EXTENDING/#adding-new-question-categories","title":"Adding New Question Categories","text":""},{"location":"EXTENDING/#for-progressive-levels-python-defined","title":"For Progressive Levels (Python-defined)","text":"<p>Add a new level to <code>src/amplihack_eval/data/progressive_levels.py</code>:</p> <pre><code>from amplihack_eval.data.progressive_levels import TestLevel, TestArticle, TestQuestion\n\nLEVEL_17 = TestLevel(\n    level_id=\"L17\",\n    level_name=\"Analogical Reasoning\",\n    description=\"Drawing analogies between structurally similar scenarios in different domains\",\n    articles=[\n        TestArticle(\n            title=\"How Ant Colonies Solve Optimization Problems\",\n            content=(\n                \"Ant colonies use pheromone trails to find shortest paths to food. \"\n                \"When a shorter path exists, more ants use it (stronger pheromone), \"\n                \"creating a positive feedback loop. This is analogous to ...\"\n            ),\n            url=\"https://example.com/ant-colonies\",\n            published=\"2026-01-01T00:00:00Z\",\n        ),\n        # More articles...\n    ],\n    questions=[\n        TestQuestion(\n            question=\"How is the ant colony optimization similar to network routing?\",\n            expected_answer=(\n                \"Both use local decisions (ants follow pheromones, packets follow routing tables) \"\n                \"that collectively find globally optimal paths through positive feedback loops.\"\n            ),\n            level=\"L17\",\n            reasoning_type=\"analogical_mapping\",\n        ),\n        # More questions...\n    ],\n)\n</code></pre> <p>Then add the level to the appropriate group list and re-export from <code>levels/__init__.py</code>.</p>"},{"location":"EXTENDING/#for-yaml-defined-levels","title":"For YAML-Defined Levels","text":"<p>Create a YAML file in <code>src/amplihack_eval/levels/</code>:</p> <pre><code># L17_analogical_reasoning.yaml\nid: \"L17\"\nname: \"Analogical Reasoning\"\ndescription: \"Drawing analogies between structurally similar scenarios\"\ncategory: \"reasoning\"\ndifficulty: 4\ndata_source: \"progressive_levels\"\nmin_turns: 50\ngrading_mode: \"hybrid\"\n\nscoring:\n  pass_threshold: 0.6\n  dimensions:\n    - factual_accuracy\n    - reasoning_quality\n  weights:\n    factual_accuracy: 0.4\n    reasoning_quality: 0.6\n  grader_votes: 3\n\nquestions:\n  - id: \"L17_Q01\"\n    text: \"How is the ant colony optimization similar to network routing?\"\n    category: \"analogical_mapping\"\n    expected_answer: \"Both use local decisions with positive feedback loops...\"\n    scoring_dimensions:\n      - factual_accuracy\n      - reasoning_quality\n    rubric:\n      required_keywords: [\"pheromone\", \"routing\", \"feedback\"]\n      acceptable_paraphrases: [\"positive feedback\", \"reinforcement\"]\n</code></pre> <p>YAML levels are automatically discovered by <code>load_all_levels()</code>.</p>"},{"location":"EXTENDING/#for-long-horizon-categories","title":"For Long-Horizon Categories","text":"<p>To add a new question category to the long-horizon evaluation, modify <code>generate_questions()</code> in <code>src/amplihack_eval/data/long_horizon.py</code>:</p> <pre><code>def generate_questions(ground_truth: GroundTruth, num_questions: int = 100) -&gt; list[Question]:\n    questions = []\n\n    # ... existing categories ...\n\n    # Add your new category\n    # Example: \"entity_count\" - questions about how many entities of a type exist\n    for entity_type in [\"people\", \"projects\", \"incidents\"]:\n        entity_facts = [\n            f for f in ground_truth.facts_by_entity.items()\n            if any(entity_type in str(v) for v in f[1])\n        ]\n        if entity_facts:\n            questions.append(Question(\n                question_id=f\"entity_count_{entity_type}_001\",\n                text=f\"How many distinct {entity_type} have been discussed?\",\n                expected_answer=str(len(entity_facts)),\n                category=\"entity_count\",\n                relevant_turns=[],  # Meta-question, all turns relevant\n                scoring_dimensions=[\"factual_accuracy\"],\n                rubric=GradingRubric(\n                    required_keywords=[str(len(entity_facts))],\n                ),\n            ))\n\n    return questions[:num_questions]\n</code></pre>"},{"location":"EXTENDING/#creating-custom-data-generators","title":"Creating Custom Data Generators","text":""},{"location":"EXTENDING/#template-based-generator-recommended","title":"Template-Based Generator (Recommended)","text":"<p>Follow the pattern used by <code>long_horizon.py</code> -- template-based, deterministic, seeded:</p> <pre><code>import random\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass MyScenario:\n    \"\"\"A single evaluation scenario.\"\"\"\n    scenario_id: str\n    context: str          # Content for agent.learn()\n    question: str         # Question for agent.answer()\n    expected_answer: str  # Ground truth\n    category: str\n    metadata: dict = field(default_factory=dict)\n\n\ndef generate_my_scenarios(num_scenarios: int = 50, seed: int = 42) -&gt; list[MyScenario]:\n    \"\"\"Generate evaluation scenarios deterministically.\n\n    Same seed = same output. No LLM needed.\n    \"\"\"\n    rng = random.Random(seed)\n    scenarios = []\n\n    # Define templates\n    templates = [\n        {\n            \"context_template\": \"The {entity} was created on {date} by {creator}.\",\n            \"question_template\": \"When was {entity} created?\",\n            \"answer_template\": \"{date}\",\n            \"category\": \"date_recall\",\n        },\n        # More templates...\n    ]\n\n    entities = [\"Project Alpha\", \"System Beta\", \"Service Gamma\"]\n    dates = [\"2024-01-15\", \"2024-03-22\", \"2024-07-01\"]\n    creators = [\"Alice\", \"Bob\", \"Carol\"]\n\n    for i in range(num_scenarios):\n        template = rng.choice(templates)\n        entity = rng.choice(entities)\n        date = rng.choice(dates)\n        creator = rng.choice(creators)\n\n        scenarios.append(MyScenario(\n            scenario_id=f\"custom_{i:04d}\",\n            context=template[\"context_template\"].format(\n                entity=entity, date=date, creator=creator\n            ),\n            question=template[\"question_template\"].format(entity=entity),\n            expected_answer=template[\"answer_template\"].format(date=date),\n            category=template[\"category\"],\n        ))\n\n    return scenarios\n</code></pre>"},{"location":"EXTENDING/#integrating-with-the-evalrunner","title":"Integrating with the EvalRunner","text":"<p>To use custom scenarios with the existing runner:</p> <pre><code>from amplihack_eval import EvalRunner\n\n# Generate your scenarios\nscenarios = generate_my_scenarios(num_scenarios=50, seed=42)\n\n# Create agent\nagent = MyAgent()\n\n# Feed scenarios as dialogue turns\nfor scenario in scenarios:\n    agent.learn(scenario.context)\n\n# Quiz and grade\nresults = []\nfor scenario in scenarios:\n    response = agent.answer(scenario.question)\n    # Use the built-in grader or your own\n    score = grade_answer(\n        response.answer,\n        scenario.expected_answer,\n        level=scenario.category,\n    )\n    results.append((scenario.scenario_id, score))\n</code></pre>"},{"location":"EXTENDING/#adding-new-grading-dimensions","title":"Adding New Grading Dimensions","text":""},{"location":"EXTENDING/#adding-to-the-hybrid-grader","title":"Adding to the Hybrid Grader","text":"<p>The hybrid grader in <code>core/runner.py</code> supports arbitrary dimension names. To add a new dimension:</p> <ol> <li>Add the dimension name to your question's <code>scoring_dimensions</code>:</li> </ol> <pre><code>Question(\n    ...,\n    scoring_dimensions=[\"factual_accuracy\", \"specificity\", \"my_new_dimension\"],\n)\n</code></pre> <ol> <li>For deterministic grading, add logic in <code>_deterministic_grade()</code>:</li> </ol> <pre><code>def _deterministic_grade(\n    answer: str, expected: str, rubric: GradingRubric, dimension: str\n) -&gt; float | None:\n    if dimension == \"my_new_dimension\":\n        # Your deterministic scoring logic\n        if \"important_keyword\" in answer.lower():\n            return 1.0\n        return 0.0\n    # ... existing dimensions ...\n</code></pre> <ol> <li>For LLM grading, the dimension name is automatically included in the grading prompt. The LLM is instructed to score on a 0.0--1.0 scale. You can enhance the prompt with dimension-specific instructions:</li> </ol> <pre><code># In the grading prompt builder\nDIMENSION_DESCRIPTIONS = {\n    \"factual_accuracy\": \"Does the answer contain the correct facts?\",\n    \"specificity\": \"Does it include specific names, numbers, dates?\",\n    \"my_new_dimension\": \"Does the answer demonstrate deep analytical thinking?\",\n}\n</code></pre>"},{"location":"EXTENDING/#adding-a-level-specific-scorer-l13-l16-pattern","title":"Adding a Level-Specific Scorer (L13--L16 Pattern)","text":"<p>For new levels that need custom scoring logic, create a scorer module:</p> <pre><code># src/amplihack_eval/levels/L17_analogical_reasoning.py\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass AnalogyScore:\n    \"\"\"Score for a single analogical reasoning scenario.\"\"\"\n    scenario_id: str\n    structural_mapping: float   # 0.0-1.0: correct structure mapped\n    domain_bridging: float      # 0.0-1.0: connected domains correctly\n    novel_inference: float      # 0.0-1.0: generated new insights from analogy\n    overall: float              # Weighted average\n\ndef score_analogy(\n    expected_mapping: dict[str, str],\n    actual_answer: str,\n    source_domain: str,\n    target_domain: str,\n) -&gt; AnalogyScore:\n    \"\"\"Score an analogical reasoning response.\"\"\"\n    # Your scoring logic here\n    ...\n</code></pre>"},{"location":"EXTENDING/#testing-your-extensions","title":"Testing Your Extensions","text":"<p>Follow the testing patterns in <code>tests/</code>:</p> <pre><code># tests/test_my_extension.py\nimport pytest\nfrom amplihack_eval.adapters.base import AgentAdapter, AgentResponse\n\nclass TestMyAdapter:\n    def test_learn_and_answer(self):\n        agent = MyAgent()\n        agent.learn(\"The sky is blue.\")\n        resp = agent.answer(\"What color is the sky?\")\n        assert isinstance(resp, AgentResponse)\n        assert \"blue\" in resp.answer.lower()\n\n    def test_reset_clears_state(self):\n        agent = MyAgent()\n        agent.learn(\"fact 1\")\n        agent.reset()\n        resp = agent.answer(\"Tell me fact 1\")\n        assert \"don't know\" in resp.answer.lower() or resp.answer == \"\"\n\n    def test_capabilities(self):\n        agent = MyAgent()\n        assert \"memory\" in agent.capabilities\n\nclass TestMyScenarios:\n    def test_all_scenarios_valid(self):\n        for s in ALL_MY_SCENARIOS:\n            assert s.scenario_id\n            assert s.question\n            assert s.expected_answer\n            assert 1 &lt;= s.difficulty &lt;= 5\n\n    def test_get_by_id(self):\n        s = get_my_scenario_by_id(\"my_001\")\n        assert s is not None\n        assert s.domain == \"engineering\"\n</code></pre> <p>Run tests:</p> <pre><code>uv run pytest tests/ -v\n</code></pre>"},{"location":"EXTENDING/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Deterministic data: Always use seeded random for data generation. No LLM in data generation.</p> </li> <li> <p>Clear ground truth: Every question must have an unambiguous expected answer.</p> </li> <li> <p>Isolated levels: Each level should test one specific capability. Do not combine too many skills in one level.</p> </li> <li> <p>Dimension alignment: Use <code>factual_accuracy</code> for objective facts, <code>specificity</code> for detail level, custom dimensions for nuanced skills.</p> </li> <li> <p>Test at multiple scales: Run with 20, 100, and 500 questions to verify your category works at different scales.</p> </li> <li> <p>Document the category: Add an entry to <code>CATEGORIES.md</code> explaining what it tests, why it matters, and example questions.</p> </li> </ol>"},{"location":"LONG_HORIZON_EVAL/","title":"Long-Horizon Memory Evaluation","text":""},{"location":"LONG_HORIZON_EVAL/#what-it-tests","title":"What It Tests","text":"<p>The long-horizon memory evaluation is a stress test for AI agent memory systems. It generates a structured dialogue of up to 5000 turns, feeds each turn to the agent's <code>learn()</code> method, then quizzes the agent on details from various points in the conversation. The goal is to measure how well an agent retains, organizes, and retrieves information at scale -- far beyond what fits in a single context window.</p> <p>Key capabilities tested: - Needle-in-haystack retrieval: Finding specific facts buried among thousands of turns - Temporal evolution tracking: Understanding how values change over time - Numerical precision: Exact recall of numbers, percentages, and metrics - Source attribution: Correctly attributing facts to their original sources - Cross-referencing: Connecting facts across different information blocks - Distractor resistance: Ignoring irrelevant information when answering - Meta-memory: Reasoning about what the agent knows (and does not know) - Security log analysis: Detecting patterns and anomalies in structured logs - Incident tracking: Following evolving incident timelines - Infrastructure knowledge: Recalling configuration details - Problem solving: Applying stored knowledge to solve new problems - Multi-hop reasoning: Chaining multiple facts to answer complex questions</p>"},{"location":"LONG_HORIZON_EVAL/#how-dialogue-generation-works","title":"How Dialogue Generation Works","text":""},{"location":"LONG_HORIZON_EVAL/#deterministic-generation","title":"Deterministic Generation","text":"<p>All dialogue content is template-based. No LLM is needed for data generation. The same seed always produces identical output:</p> <pre><code>from amplihack_eval.data.long_horizon import generate_dialogue, generate_questions\n\ngt = generate_dialogue(num_turns=1000, seed=42)   # Deterministic\nquestions = generate_questions(gt, num_questions=50)\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#the-12-information-blocks","title":"The 12 Information Blocks","text":"<p>The dialogue is divided into 12 thematic blocks, each allocated a proportional range of the total turns. For a 5000-turn dialogue:</p> <pre><code>Block  1: People              (turns    1-  250,  5%)  -- personal details, preferences\nBlock  2: Projects            (turns  251-  750, 10%)  -- project updates with changes\nBlock  3: Technical           (turns  751- 1250, 10%)  -- technical facts across 9 domains\nBlock  4: Evolving Story      (turns 1251- 2000, 15%)  -- story with corrections/updates\nBlock  5: Numerical           (turns 2001- 2500, 10%)  -- precise metrics and KPIs\nBlock  6: Contradictory       (turns 2501- 2900,  8%)  -- conflicting reports from sources\nBlock  7: Callbacks           (turns 2901- 3200,  6%)  -- references back to earlier blocks\nBlock  8: Distractors         (turns 3201- 3500,  6%)  -- irrelevant fun facts\nBlock  9: Security Logs       (turns 3501- 4000, 10%)  -- structured security events\nBlock 10: Incidents           (turns 4001- 4400,  8%)  -- incident reports with status updates\nBlock 11: Infrastructure      (turns 4401- 4750,  7%)  -- server/network inventory\nBlock 12: Problem Solving     (turns 4751- 5000,  5%)  -- problem descriptions with solutions\n</code></pre> <p>Turn counts scale linearly. A 100-turn dialogue uses 1/50th of each range.</p>"},{"location":"LONG_HORIZON_EVAL/#block-details","title":"Block Details","text":"<p>Block 1: People -- 10 team members with detailed personal profiles (name, birthday, allergy, hobby, role, team, pet, hometown, favorite food, degree). Each person's facts are delivered across multiple turns with natural-language context.</p> <p>Block 2: Projects -- 5 projects (Atlas, Beacon, Cascade, Delta, Echo) each with initial descriptions and a series of updates that change deadlines, budgets, team sizes, and project leads at specific turn offsets. This tests temporal evolution tracking.</p> <p>Block 3: Technical -- Facts from 9 technical domains: programming, security, databases, cloud, ML/AI, DevOps, architecture, frontend. Each fact is a standalone technical statement (e.g., \"PostgreSQL 16 improved parallel query performance by 40%\").</p> <p>Block 4: Evolving Story -- A multi-chapter narrative about a startup's journey with deliberate corrections and updates. Tests the agent's ability to track the most current version of facts.</p> <p>Block 5: Numerical -- 30 precise metrics (Q1 revenue, server uptime, test coverage, API response times, etc.) with specific values and context details. Tests numerical precision.</p> <p>Block 6: Contradictory -- 8 topics where 2-3 different sources provide conflicting claims (e.g., Q3 revenue: Finance says $5.2M, Auditor says $4.8M, Board says $5.0M). Tests the agent's ability to acknowledge and reason about contradictions.</p> <p>Block 7: Callbacks -- References back to facts from earlier blocks, creating cross-references that require connecting information across different topics.</p> <p>Block 8: Distractors -- 30 irrelevant fun facts (e.g., \"Octopuses have three hearts and blue blood\") designed to test whether the agent can filter relevant from irrelevant information.</p> <p>Block 9: Security Logs -- Structured security events with timestamps, source IPs, event types, users, and severity levels. Includes attack patterns (brute force SSH, SQL injection, data exfiltration, C2 communication) that require pattern recognition.</p> <p>Block 10: Incidents -- Incident reports with evolving status updates (open -&gt; investigating -&gt; identified -&gt; resolved). Each incident has a timeline of events that tests temporal tracking.</p> <p>Block 11: Infrastructure -- Server and network inventory with detailed specifications (CPU, RAM, storage, OS, location, uptime).</p> <p>Block 12: Problem Solving -- Problem descriptions paired with solutions, testing the agent's ability to recall and apply stored problem-solving knowledge.</p>"},{"location":"LONG_HORIZON_EVAL/#ground-truth-tracking","title":"Ground Truth Tracking","text":"<p>Every fact delivered to the agent is tracked in a <code>GroundTruth</code> structure:</p> <pre><code>@dataclass\nclass GroundTruth:\n    turns: list[Turn]                           # All dialogue turns\n    facts_by_entity: dict[str, list[dict]]      # Facts indexed by entity name\n    current_values: dict[str, Any]              # Latest value for each entity\n    superseded_values: dict[str, list[dict]]    # Historical values with timestamps\n</code></pre> <p>Each <code>Turn</code> records: - <code>turn_number</code> -- position in the dialogue - <code>content</code> -- the text delivered to the agent - <code>block</code> -- which block (1-12) this turn belongs to - <code>block_name</code> -- human-readable block name - <code>facts</code> -- list of ground truth facts delivered in this turn</p>"},{"location":"LONG_HORIZON_EVAL/#the-15-question-categories","title":"The 15 Question Categories","text":"<p>Questions are generated from the ground truth data. Each question has an expected answer, relevant turn numbers, scoring dimensions, and an optional deterministic grading rubric.</p>"},{"location":"LONG_HORIZON_EVAL/#1-needle_in_haystack","title":"1. <code>needle_in_haystack</code>","text":"<p>What it tests: Direct recall of specific facts from a single source among many turns.</p> <p>Example: \"What is Sarah Chen's allergy?\" (Expected: \"shellfish\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: The most fundamental memory capability. If an agent cannot find a specific fact in its memory, nothing else works.</p>"},{"location":"LONG_HORIZON_EVAL/#2-temporal_evolution","title":"2. <code>temporal_evolution</code>","text":"<p>What it tests: Tracking how values change over time, including computing differences.</p> <p>Example: \"What is the current deadline for Project Atlas, and how many times has it changed?\" (Expected: \"September 20, changed twice: June 15 -&gt; August 3 -&gt; September 20\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>temporal_awareness</code></p> <p>Why it matters: Real-world information evolves constantly. Agents must track the current state while being aware of historical changes.</p>"},{"location":"LONG_HORIZON_EVAL/#3-numerical_precision","title":"3. <code>numerical_precision</code>","text":"<p>What it tests: Exact recall of numbers, percentages, and metrics.</p> <p>Example: \"What is the Q1 revenue and how does it compare to the forecast?\" (Expected: \"$4.7M, 12% above forecast of $4.2M\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: Approximate recall is insufficient for financial data, metrics, and technical specifications.</p>"},{"location":"LONG_HORIZON_EVAL/#4-source_attribution","title":"4. <code>source_attribution</code>","text":"<p>What it tests: Correctly attributing claims to their original sources, especially when multiple sources discuss the same topic.</p> <p>Example: \"According to the Finance Department, what is Q3 revenue?\" (Expected: \"$5.2M, includes deferred revenue\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>source_attribution</code></p> <p>Why it matters: In environments with multiple information sources, knowing who said what is as important as knowing the facts themselves.</p>"},{"location":"LONG_HORIZON_EVAL/#5-cross_reference","title":"5. <code>cross_reference</code>","text":"<p>What it tests: Connecting facts across different information blocks.</p> <p>Example: \"Sarah Chen leads Project Atlas. What is her educational background?\" (requires connecting Block 2 project data with Block 1 people data)</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: Real-world knowledge is interconnected. Agents must link related facts across different contexts.</p>"},{"location":"LONG_HORIZON_EVAL/#6-distractor_resistance","title":"6. <code>distractor_resistance</code>","text":"<p>What it tests: Answering questions accurately while ignoring irrelevant information.</p> <p>Example: \"What is the company's Q1 revenue? Note: do not confuse with the fact that honey never spoils.\" (Expected: \"$4.7M\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>confidence_calibration</code></p> <p>Why it matters: Memory systems that retrieve by similarity can be fooled by distractors. This tests precision of retrieval.</p>"},{"location":"LONG_HORIZON_EVAL/#7-meta_memory","title":"7. <code>meta_memory</code>","text":"<p>What it tests: The agent's awareness of what it knows and does not know.</p> <p>Example: \"How many distinct people have you been told about?\" (Expected: count of people from Block 1)</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>confidence_calibration</code></p> <p>Why it matters: Agents that can reason about their own knowledge are more reliable and trustworthy.</p>"},{"location":"LONG_HORIZON_EVAL/#8-security_log_analysis","title":"8. <code>security_log_analysis</code>","text":"<p>What it tests: Pattern recognition in structured security event data.</p> <p>Example: \"What IP address was involved in the brute force SSH attack?\" (Expected: \"192.168.1.45\")</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: Security analysis requires precise recall of structured data including IPs, timestamps, and event sequences.</p>"},{"location":"LONG_HORIZON_EVAL/#9-incident_tracking","title":"9. <code>incident_tracking</code>","text":"<p>What it tests: Following incident timelines and status evolution.</p> <p>Example: \"What is the current status of Incident INC-001, and when was the root cause identified?\"</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>temporal_awareness</code></p> <p>Why it matters: Incident management requires tracking evolving states across multiple updates.</p>"},{"location":"LONG_HORIZON_EVAL/#10-infrastructure_knowledge","title":"10. <code>infrastructure_knowledge</code>","text":"<p>What it tests: Recall of technical infrastructure details.</p> <p>Example: \"What is the CPU specification of server web-prod-01?\" (Expected: specific CPU model and specs)</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: Infrastructure queries require exact recall of configuration details.</p>"},{"location":"LONG_HORIZON_EVAL/#11-problem_solving","title":"11. <code>problem_solving</code>","text":"<p>What it tests: Recalling stored problem-solution pairs and applying them.</p> <p>Example: \"A user reports their application is running slowly after a deployment. What was the recommended diagnostic approach?\"</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: The ultimate purpose of stored knowledge is application to solve problems.</p>"},{"location":"LONG_HORIZON_EVAL/#12-multi_hop_reasoning","title":"12. <code>multi_hop_reasoning</code>","text":"<p>What it tests: Chaining multiple facts to answer a question that requires 2+ retrieval steps.</p> <p>Example: \"The lead of Project Echo was recently changed. What team is the new lead originally from?\" (requires: Echo's new lead -&gt; that person's team)</p> <p>Scoring dimensions: <code>factual_accuracy</code>, <code>specificity</code></p> <p>Why it matters: Complex questions rarely have answers in a single fact. Multi-hop reasoning tests the agent's ability to compose knowledge.</p>"},{"location":"LONG_HORIZON_EVAL/#13-15-additional-categories-generated-from-blocks","title":"13-15. Additional Categories (generated from blocks)","text":"<p>The question generator also produces additional questions that combine categories -- for example, temporal + numerical (\"What was the budget change for Project Atlas and when did it happen?\") or cross-reference + security (\"Which team member from the Security team had events logged in the security block?\").</p>"},{"location":"LONG_HORIZON_EVAL/#the-grading-system","title":"The Grading System","text":""},{"location":"LONG_HORIZON_EVAL/#hybrid-deterministic-llm-grading","title":"Hybrid Deterministic + LLM Grading","text":"<p>Each question is graded on multiple dimensions using a two-tier approach:</p> <p>Tier 1 -- Deterministic grading (instant, free, reproducible): - Checks <code>required_keywords</code> against the answer (case-insensitive) - Awards bonus for <code>acceptable_paraphrases</code> found - Scores 0.0 if any <code>incorrect_patterns</code> match - Applies to <code>factual_accuracy</code> and <code>specificity</code> dimensions</p> <p>Tier 2 -- LLM semantic grading (slower, costs API calls, nuanced): - Used for <code>temporal_awareness</code>, <code>source_attribution</code>, <code>confidence_calibration</code> - Also used when no deterministic rubric is available - Prompt includes scoring guide and dimension descriptions - Returns structured JSON with per-dimension scores and reasoning</p>"},{"location":"LONG_HORIZON_EVAL/#multi-vote-stability","title":"Multi-Vote Stability","text":"<p>To reduce LLM grading noise, each question is graded N times (configurable, default 3). For each dimension, the median score is taken as the final grade. The reasoning from the vote closest to the median is preserved.</p> <pre><code>Vote 1: factual_accuracy = 0.85\nVote 2: factual_accuracy = 0.90    -&gt; Median: 0.85\nVote 3: factual_accuracy = 0.80\n</code></pre> <p>For deterministic dimensions, multi-vote has zero overhead since the score is identical every time.</p>"},{"location":"LONG_HORIZON_EVAL/#dimension-weights","title":"Dimension Weights","text":"<p>Questions can specify custom dimension weights via their rubric:</p> <pre><code>GradingRubric(\n    required_keywords=[\"Paris\"],\n    dimension_weights={\"factual_accuracy\": 1.5, \"specificity\": 0.5}\n)\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#scoring-scale","title":"Scoring Scale","text":"Score Meaning 1.0 Perfect or semantically equivalent 0.8-0.9 Correct main points, minor differences 0.5-0.7 Partially correct, missing key details 0.2-0.4 Some relevant content, significant gaps 0.0-0.1 Incorrect or irrelevant"},{"location":"LONG_HORIZON_EVAL/#how-to-interpret-results","title":"How to Interpret Results","text":""},{"location":"LONG_HORIZON_EVAL/#the-evalreport","title":"The EvalReport","text":"<p>A completed evaluation produces an <code>EvalReport</code> containing:</p> <pre><code>@dataclass\nclass EvalReport:\n    num_turns: int                          # Dialogue length\n    num_questions: int                      # Questions asked\n    total_facts_delivered: int              # Total facts in ground truth\n    learning_time_s: float                  # Time to feed all turns\n    questioning_time_s: float               # Time to ask + grade all questions\n    grading_time_s: float                   # Time spent on grading only\n    overall_score: float                    # Average of all question scores\n    category_breakdown: list[CategoryBreakdown]  # Per-category averages\n    results: list[EvalResult]              # Per-question details\n    memory_stats: dict                     # Agent-reported memory statistics\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#reading-the-category-breakdown","title":"Reading the Category Breakdown","text":"<pre><code>CATEGORY BREAKDOWN:\n-----------------------------------------------------------------------\nCategory                     Avg      Min      Max   Count\n-----------------------------------------------------------------------\ncross_reference            85.00%   70.00%   95.00%      10\ndistractor_resistance      92.00%   80.00%  100.00%       8\nmeta_memory                78.00%   60.00%   90.00%       5\nneedle_in_haystack         88.00%   65.00%  100.00%      20\nnumerical_precision        82.00%   55.00%   95.00%      15\nsecurity_log_analysis      90.00%   80.00%  100.00%       8\nsource_attribution         75.00%   50.00%   90.00%      10\ntemporal_evolution          70.00%   40.00%   90.00%      15\n</code></pre> <p>Focus on the weakest categories. Categories below 70% indicate systematic weaknesses in the agent's memory system. The min score reveals worst-case performance.</p>"},{"location":"LONG_HORIZON_EVAL/#dimension-averages","title":"Dimension Averages","text":"<pre><code>DIMENSION AVERAGES BY CATEGORY:\n  needle_in_haystack: factual_accuracy: 90%, specificity: 86%\n  temporal_evolution: factual_accuracy: 75%, temporal_awareness: 65%\n  source_attribution: factual_accuracy: 80%, source_attribution: 70%\n</code></pre> <p>If <code>factual_accuracy</code> is high but <code>temporal_awareness</code> is low, the agent can recall facts but struggles with temporal ordering. If <code>source_attribution</code> is low, the agent retrieves facts but loses track of their origins.</p>"},{"location":"LONG_HORIZON_EVAL/#the-worst-5-questions","title":"The Worst 5 Questions","text":"<p>The report highlights the 5 lowest-scoring questions. These are the best starting points for debugging:</p> <pre><code>WORST 5 QUESTIONS:\n  [25.00%] What was the budget change for Project Atlas over time?\n    Expected: $2.1M -&gt; $2.5M (turn 45, additional cloud credits needed)\n    Got: The budget is $2.5M.\n</code></pre> <p>This example shows the agent knows the current value but lost the change history -- a temporal awareness issue.</p>"},{"location":"LONG_HORIZON_EVAL/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"LONG_HORIZON_EVAL/#scaling","title":"Scaling","text":"Turns Questions Typical Facts Generation Time Learning Time* 100 20 ~80 &lt; 0.1s Depends on agent 500 50 ~400 &lt; 0.5s Depends on agent 1000 100 ~800 &lt; 1s Depends on agent 5000 200 ~4000 &lt; 5s Depends on agent <p>*Learning time depends entirely on the agent implementation -- a simple list-based agent will be much faster than one using LLM-powered ingestion.</p>"},{"location":"LONG_HORIZON_EVAL/#grading-costs","title":"Grading Costs","text":"<ul> <li>Deterministic grading: Free, instant (no API calls)</li> <li>LLM grading: 1 API call per dimension per vote per question</li> <li>Example: 100 questions * 3 LLM dimensions * 3 votes = 900 API calls</li> <li>Typical cost: ~$0.50 for 100 questions with 3-vote grading (Sonnet model)</li> </ul>"},{"location":"LONG_HORIZON_EVAL/#reproducibility","title":"Reproducibility","text":"<p>Same seed + same agent = same scores (within LLM grading variance). Multi-vote and multi-seed evaluation reduce this variance. For fully deterministic results, set all question rubrics and use <code>grading_mode=\"deterministic\"</code>.</p>"},{"location":"LONG_HORIZON_EVAL/#pre-built-datasets","title":"Pre-built Datasets","text":""},{"location":"LONG_HORIZON_EVAL/#skip-the-4-hour-learning-phase","title":"Skip the 4+ Hour Learning Phase","text":"<p>The 5000-turn learning phase takes 4+ hours and requires 10,000+ LLM API calls. Pre-built datasets let you skip this entirely and jump straight to evaluation.</p>"},{"location":"LONG_HORIZON_EVAL/#available-datasets","title":"Available Datasets","text":"Name Turns Seed Facts Baseline Score Size <code>5000t-seed42-v1.0</code> 5,000 42 762 90.47% 1.3 MB"},{"location":"LONG_HORIZON_EVAL/#download-and-use","title":"Download and Use","text":"<pre><code># List available datasets\namplihack-eval list-datasets\n\n# Download a pre-built dataset\namplihack-eval download-dataset 5000t-seed42-v1.0\n\n# Run evaluation using the pre-built DB (skip 4+ hour learning phase)\namplihack-eval run \\\n  --adapter learning-agent \\\n  --skip-learning \\\n  --load-db datasets/5000t-seed42-v1.0/memory_db \\\n  --turns 5000 \\\n  --questions 100\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from amplihack_eval.datasets import download_dataset, list_datasets\n\n# List available datasets\ndatasets = list_datasets()\nfor ds in datasets:\n    print(f\"{ds['name']}: {'local' if ds.get('local') else 'remote'}\")\n\n# Download a dataset\npath = download_dataset(\"5000t-seed42-v1.0\")\n\n# Use with evaluation\nfrom amplihack_eval.adapters.learning_agent import LearningAgentAdapter\nadapter = LearningAgentAdapter(storage_path=path / \"memory_db\")\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#dataset-structure","title":"Dataset Structure","text":"<p>Each dataset contains: - <code>metadata.json</code> -- Configuration and provenance (turns, seed, facts, baseline score, code version) - <code>baseline_results.json</code> -- Full evaluation scores at time of creation - <code>memory_db/</code> -- Kuzu graph database (the pre-built learning DB)</p> <p>Datasets are distributed via GitHub Releases to keep the repository lightweight.</p>"},{"location":"LONG_HORIZON_EVAL/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic evaluation (100 turns, 20 questions)\namplihack-eval run --turns 100 --questions 20 --adapter http --agent-url http://localhost:8000\n\n# Large-scale stress test\namplihack-eval run --turns 5000 --questions 200 --grader-votes 5 --seed 42\n\n# Skip learning with pre-built DB\namplihack-eval run --adapter learning-agent --skip-learning \\\n  --load-db datasets/5000t-seed42-v1.0/memory_db --turns 5000 --questions 100\n\n# Multi-seed comparison\namplihack-eval compare --seeds 42,123,456,789 --turns 100 --questions 20\n\n# With LearningAgent\namplihack-eval run --turns 100 --adapter learning-agent --model claude-sonnet-4-5-20250929\n\n# Dataset management\namplihack-eval list-datasets\namplihack-eval download-dataset 5000t-seed42-v1.0\n</code></pre>"},{"location":"LONG_HORIZON_EVAL/#programmatic-usage_1","title":"Programmatic Usage","text":"<pre><code>from amplihack_eval import EvalRunner\n\n# Create your agent (must implement AgentAdapter)\nagent = MyAgent()\n\n# Run evaluation\nrunner = EvalRunner(num_turns=1000, num_questions=100, seed=42, grader_votes=3)\nreport = runner.run(agent, grader_model=\"claude-sonnet-4-5-20250929\")\n\n# Inspect results\nprint(f\"Overall: {report.overall_score:.2%}\")\nfor cb in report.category_breakdown:\n    print(f\"  {cb.category}: {cb.avg_score:.2%} (n={cb.num_questions})\")\n\n# Save report\nimport json\nwith open(\"report.json\", \"w\") as f:\n    json.dump(report.to_dict(), f, indent=2)\n</code></pre>"},{"location":"adapters/","title":"Writing Custom Agent Adapters","text":""},{"location":"adapters/#the-agentadapter-interface","title":"The AgentAdapter Interface","text":"<p>To make any agent evaluable, implement the <code>AgentAdapter</code> abstract base class from <code>amplihack_eval.adapters.base</code>:</p> <pre><code>from amplihack_eval import AgentAdapter, AgentResponse, ToolCall\n\nclass MyAgent(AgentAdapter):\n    def learn(self, content: str) -&gt; None:\n        \"\"\"Feed content to the agent for learning/memorization.\"\"\"\n        ...\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        \"\"\"Ask the agent a question. Returns answer + trajectory.\"\"\"\n        ...\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset agent state between eval runs.\"\"\"\n        ...\n\n    def close(self) -&gt; None:\n        \"\"\"Clean up resources (connections, files, etc.).\"\"\"\n        ...\n</code></pre>"},{"location":"adapters/#required-methods","title":"Required Methods","text":"Method Signature Purpose <code>learn</code> <code>(content: str) -&gt; None</code> Feed content to the agent. Called once per dialogue turn during evaluation. <code>answer</code> <code>(question: str) -&gt; AgentResponse</code> Ask the agent a question. Must return an <code>AgentResponse</code>. <code>reset</code> <code>() -&gt; None</code> Reset all agent state. Called between evaluation runs. <code>close</code> <code>() -&gt; None</code> Clean up resources. Called when evaluation is complete."},{"location":"adapters/#optional-properties","title":"Optional Properties","text":"Property Type Default Purpose <code>capabilities</code> <code>set[str]</code> <code>{\"memory\"}</code> Declare what the agent can do. Used by the runner to select appropriate eval levels. <code>name</code> <code>str</code> Class name Human-readable name for reports and logs."},{"location":"adapters/#agentresponse","title":"AgentResponse","text":"<p>The <code>answer()</code> method must return an <code>AgentResponse</code>:</p> <pre><code>@dataclass\nclass AgentResponse:\n    answer: str                              # Required: the agent's answer text\n    tool_calls: list[ToolCall] = []          # Optional: tool invocations\n    reasoning_trace: str = \"\"                # Optional: chain-of-thought\n    confidence: float = 0.0                  # Optional: self-reported confidence\n    metadata: dict[str, Any] = {}            # Optional: arbitrary metadata\n</code></pre>"},{"location":"adapters/#toolcall","title":"ToolCall","text":"<p>If your agent uses tools, capture them for trajectory analysis:</p> <pre><code>@dataclass\nclass ToolCall:\n    tool_name: str               # Name of the tool invoked\n    arguments: dict[str, Any]    # Arguments passed to the tool\n    result: str                  # String result from the tool\n    timestamp: float = 0.0       # Optional: when the call happened\n</code></pre>"},{"location":"adapters/#built-in-adapters","title":"Built-in Adapters","text":""},{"location":"adapters/#httpadapter","title":"HttpAdapter","text":"<p>For agents exposed via REST API:</p> <pre><code>from amplihack_eval.adapters.http_adapter import HttpAdapter\n\nadapter = HttpAdapter(\n    base_url=\"http://localhost:8000\",\n    timeout=30,\n)\n</code></pre> <p>Expected endpoints: - <code>POST /learn</code> with <code>{\"content\": \"...\"}</code> -&gt; 200 OK - <code>POST /answer</code> with <code>{\"question\": \"...\"}</code> -&gt; <code>{\"answer\": \"...\", \"tool_calls\": [...], ...}</code> - <code>POST /reset</code> -&gt; 200 OK</p>"},{"location":"adapters/#subprocessadapter","title":"SubprocessAdapter","text":"<p>For agents invokable via CLI:</p> <pre><code>from amplihack_eval.adapters.subprocess_adapter import SubprocessAdapter\n\nadapter = SubprocessAdapter(\n    command=[\"python\", \"my_agent.py\"],\n    learn_flag=\"--learn\",\n    answer_flag=\"--answer\",\n)\n</code></pre>"},{"location":"adapters/#learningagentadapter","title":"LearningAgentAdapter","text":"<p>For the amplihack LearningAgent (requires <code>amplihack</code> package):</p> <pre><code>from amplihack_eval.adapters.learning_agent import LearningAgentAdapter\n\nadapter = LearningAgentAdapter()\n</code></pre>"},{"location":"adapters/#complete-custom-adapter-example","title":"Complete Custom Adapter Example","text":"<pre><code>from amplihack_eval import AgentAdapter, AgentResponse, ToolCall\n\nclass RAGAgent(AgentAdapter):\n    \"\"\"Adapter for a retrieval-augmented generation agent.\"\"\"\n\n    def __init__(self, db_url: str, model: str = \"gpt-4\"):\n        self.db_url = db_url\n        self.model = model\n        self.client = VectorDBClient(db_url)\n        self.llm = LLMClient(model)\n\n    def learn(self, content: str) -&gt; None:\n        # Chunk and embed content into vector DB\n        chunks = self._chunk(content)\n        embeddings = self.llm.embed(chunks)\n        self.client.upsert(chunks, embeddings)\n\n    def answer(self, question: str) -&gt; AgentResponse:\n        # Retrieve relevant chunks\n        query_embedding = self.llm.embed([question])[0]\n        results = self.client.search(query_embedding, top_k=5)\n\n        # Generate answer with context\n        context = \"\\n\".join(r.text for r in results)\n        response = self.llm.generate(\n            f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n        )\n\n        return AgentResponse(\n            answer=response.text,\n            tool_calls=[\n                ToolCall(\n                    tool_name=\"vector_search\",\n                    arguments={\"query\": question, \"top_k\": 5},\n                    result=f\"Found {len(results)} chunks\",\n                )\n            ],\n            reasoning_trace=f\"Retrieved {len(results)} chunks, generated answer\",\n            confidence=response.confidence,\n        )\n\n    def reset(self) -&gt; None:\n        self.client.clear()\n\n    def close(self) -&gt; None:\n        self.client.close()\n        self.llm.close()\n\n    @property\n    def capabilities(self) -&gt; set[str]:\n        return {\"memory\", \"tool_use\"}\n\n    @property\n    def name(self) -&gt; str:\n        return f\"RAGAgent({self.model})\"\n</code></pre>"},{"location":"adapters/#running-evaluation-with-a-custom-adapter","title":"Running Evaluation with a Custom Adapter","text":"<pre><code>from amplihack_eval import EvalRunner\n\nagent = RAGAgent(db_url=\"http://localhost:6333\", model=\"gpt-4\")\nrunner = EvalRunner(num_turns=100, num_questions=20, grader_votes=3)\nreport = runner.run(agent)\n\nprint(f\"Overall: {report.overall_score:.2%}\")\nfor cb in report.category_breakdown:\n    print(f\"  {cb.category}: {cb.avg_score:.2%}\")\n\nagent.close()\n</code></pre>"},{"location":"adapters/#tips","title":"Tips","text":"<ul> <li>Keep <code>learn()</code> fast: The runner calls it once per dialogue turn (potentially 1000+ times). Batch operations if possible.</li> <li>Capture tool calls: Even if your agent does not use explicit tools, logging internal retrieval as a <code>ToolCall</code> enables richer analysis.</li> <li>Set confidence: If your agent can estimate confidence, include it. The grader uses confidence calibration in advanced eval levels (L8).</li> <li>Reset completely: <code>reset()</code> must clear ALL state. Leftover state between runs corrupts multi-seed evaluation.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p><code>amplihack-agent-eval</code> is a modular evaluation framework for goal-seeking AI agents. It measures memory recall, tool use, planning, and reasoning across progressive difficulty levels (L1--L16) and long-horizon memory stress tests (up to 5000 dialogue turns). The package is designed to be agent-agnostic: any system that can learn content and answer questions is evaluable through the <code>AgentAdapter</code> interface.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>                        +---------------------+\n                        |        CLI          |\n                        |  amplihack_eval.cli |\n                        +--------+------------+\n                                 |\n            +--------------------+--------------------+\n            |                    |                    |\n   +--------v----------+ +------v-----------+ +------v-----------+\n   |    EvalRunner      | | SelfImproveRunner| | MultiSeedRunner  |\n   |   (core/runner)    | | (self_improve/)  | | (core/multi_seed)|\n   +--------+-----------+ +------+-----------+ +---------+--------+\n            |                    |                       |\n            +--------------------+-----------+-----------+\n                                 |           |\n                        +--------v--------+  |\n                        |     Grader      |  |\n                        | (core/grader +  |&lt;-+\n                        |  runner inline) |\n                        +--------+--------+\n                                 |\n                        +--------v--------+\n                        | Data Generation |\n                        |    (data/)      |\n                        +-----------------+\n\n   +---------------------+    +-----------------------+\n   |   AgentAdapter       |    |  Multi-Agent Eval     |\n   |  (adapters/base.py)  |    | (multi_agent_eval/)   |\n   +-----+-----+-----+---+    | Coordinator, Grader,  |\n         |     |     |        | Adversary, Analyst,   |\n       HTTP  Subproc LearningAgent | Pipeline          |\n       Adapter Adapter  Adapter    +-----------------------+\n</code></pre>"},{"location":"architecture/#package-layout","title":"Package Layout","text":"<pre><code>src/amplihack_eval/\n    __init__.py                  # Public API: EvalRunner, AgentAdapter, AgentResponse, ...\n    cli.py                       # CLI entry point (amplihack-eval command)\n\n    adapters/                    # Agent adapter layer\n        __init__.py\n        base.py                  # AgentAdapter ABC, AgentResponse, ToolCall\n        http_adapter.py          # REST API adapter (POST /learn, /answer, /reset)\n        subprocess_adapter.py    # CLI subprocess adapter (stdin/stdout)\n        learning_agent.py        # amplihack LearningAgent wrapper\n\n    core/                        # Evaluation engine\n        __init__.py\n        runner.py                # EvalRunner, LevelResult, SuiteResult, run_level, run_suite\n        grader.py                # Standalone grade_answer() with multi-vote support\n        multi_seed.py            # Multi-seed holdout evaluation with variance analysis\n\n    data/                        # Test data generation\n        __init__.py              # Re-exports all data modules\n        long_horizon.py          # 5000-turn dialogue generator (12 blocks, 15 categories)\n        progressive_levels.py    # L1-L12 Python-defined levels (articles + questions)\n        tool_use_scenarios.py    # L13 tool selection scenarios\n        forgetting_scenarios.py  # L14 selective forgetting scenarios\n        adversarial_scenarios.py # L15 adversarial recall scenarios\n        decision_scenarios.py    # L16 decision-from-memory scenarios\n\n    levels/                      # Level schema, loader, and scoring modules\n        __init__.py              # Convenience re-exports\n        schema.py                # LevelDefinition, QuestionTemplate, ScoringConfig\n        loader.py                # YAML-driven level loader\n        L13_tool_selection.py    # Tool selection scoring logic\n        L14_selective_forgetting.py  # Selective forgetting scoring logic\n        L15_adversarial_recall.py    # Adversarial recall scoring logic\n        L16_decision_from_memory.py  # Decision-from-memory scoring logic\n\n    self_improve/                # Automated self-improvement loop\n        __init__.py\n        runner.py                # 8-phase self-improvement orchestrator\n        patch_proposer.py        # LLM-powered patch generation with history\n        reviewer_voting.py       # Devil's advocate + 3-reviewer A/B voting\n\n    multi_agent_eval/            # Multi-agent evaluation pipeline\n        __init__.py\n        coordinator.py           # EvalCoordinator\n        grader_agent.py          # GraderAgent with perspective-based grading\n        adversary_agent.py       # AdversaryAgent for hard question generation\n        analyst_agent.py         # AnalystAgent for failure analysis\n        pipeline.py              # MultiAgentEvalPipeline end-to-end orchestrator\n\ntests/\n    test_adapters.py             # AgentAdapter interface + concrete adapter tests\n    test_data_generation.py      # Data generator + progressive level tests\n\nrecipes/                         # YAML recipes (future, currently .gitkeep)\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-agentadapter-adaptersbasepy","title":"1. AgentAdapter (<code>adapters/base.py</code>)","text":"<p>The central abstraction. Four methods define the complete contract:</p> <pre><code>+------------------+\n|   AgentAdapter   |  (abstract base class)\n+------------------+\n| + learn(content) |  Feed content for memorization\n| + answer(question)| Ask a question, get AgentResponse\n| + reset()        |  Reset state between runs\n| + close()        |  Clean up resources\n+------------------+\n| capabilities     |  set[str] - what the agent can do\n| name             |  str - human-readable name\n+------------------+\n</code></pre> <p><code>AgentResponse</code> captures more than just the answer text. It includes: - <code>tool_calls: list[ToolCall]</code> -- the agent's tool use trajectory - <code>reasoning_trace: str</code> -- chain-of-thought or reasoning log - <code>confidence: float</code> -- self-reported confidence - <code>metadata: dict</code> -- arbitrary key-value pairs (e.g., latency, model name)</p> <p>Built-in adapters:</p> Adapter Communication Use case <code>HttpAdapter</code> REST API Any agent with HTTP endpoints <code>SubprocessAdapter</code> stdin/stdout Any CLI-invokable agent <code>LearningAgentAdapter</code> Direct import amplihack's LearningAgent"},{"location":"architecture/#2-evalrunner-corerunnerpy","title":"2. EvalRunner (<code>core/runner.py</code>)","text":"<p>Orchestrates the full evaluation pipeline: generate data, feed to agent, quiz, grade.</p> <pre><code>EvalRunner\n    |\n    |-- generate()        -&gt; (GroundTruth, list[Question])\n    |-- run_dialogue()    -&gt; feeds all turns to agent.learn()\n    |-- evaluate()        -&gt; quizzes agent, grades answers\n    |-- run()             -&gt; all three steps in sequence\n</code></pre> <p>The runner also supports YAML-driven level evaluation through <code>run_level()</code> and <code>run_suite()</code>, which load level definitions, feed articles to the agent, and grade with the level's scoring configuration.</p> <p>Key data flow:</p> <pre><code>generate_dialogue(num_turns, seed)\n         |\n         v\n    GroundTruth            # Turns with embedded facts, entity tracking\n         |\n         v\ngenerate_questions(gt, num_questions)\n         |\n         v\n    list[Question]         # Questions with expected answers, rubrics, categories\n         |\n         v\n    agent.learn(turn.content)   # Feed each turn\n         |\n         v\n    agent.answer(question.text) # Quiz the agent\n         |\n         v\n    _grade_multi_vote()    # Hybrid grading with multi-vote stability\n         |\n         v\n    EvalReport             # Aggregate scores by category and dimension\n</code></pre>"},{"location":"architecture/#3-grading-system-corerunnerpy-inline-coregraderpy","title":"3. Grading System (<code>core/runner.py</code> inline + <code>core/grader.py</code>)","text":"<p>Two complementary grading subsystems:</p> <p>Runner-integrated grading (in <code>runner.py</code>): - <code>_deterministic_grade()</code> -- regex/keyword matching against rubrics - <code>_grade_with_llm()</code> -- LLM semantic evaluation on multiple dimensions - <code>_grade_hybrid()</code> -- deterministic for rubric-compatible dimensions, LLM for the rest - <code>_grade_multi_vote()</code> -- runs hybrid grading N times, takes median per dimension</p> <p>Standalone grader (in <code>grader.py</code>): - <code>grade_answer()</code> -- independent grading function with level-specific criteria - Level-aware grading prompts (L3 temporal, L5 contradiction, L9 causal, etc.) - Multi-vote support with median aggregation</p> <p>Grading dimensions:</p> Dimension Deterministic? Description <code>factual_accuracy</code> Yes Does the answer match key facts? <code>specificity</code> Yes Does it include names, numbers, dates? <code>temporal_awareness</code> LLM only Current vs. historical value distinction <code>source_attribution</code> LLM only Correct source labeling <code>confidence_calibration</code> LLM only Appropriate uncertainty expression"},{"location":"architecture/#4-data-generation-data","title":"4. Data Generation (<code>data/</code>)","text":"<p>Long-horizon generator (<code>long_horizon.py</code>): - Produces 100--5000 turns of structured dialogue - 12 information blocks (people, projects, technical, evolving stories, numerical, contradictions, callbacks, distractors, security logs, incidents, infrastructure, problem-solving) - Deterministic: same seed produces identical output - Ground truth tracking: every fact is recorded with its delivery turn</p> <p>Progressive levels (<code>progressive_levels.py</code>): - Hand-crafted L1--L12 with curated articles and questions - Each level is a <code>TestLevel</code> dataclass with articles, questions, and metadata</p> <p>Extended scenarios (L13--L16): - <code>tool_use_scenarios.py</code> -- tool selection/chaining scenarios with expected tool sequences - <code>forgetting_scenarios.py</code> -- fact update scenarios testing stale data handling - <code>adversarial_scenarios.py</code> -- plausible-but-wrong questions testing hallucination resistance - <code>decision_scenarios.py</code> -- fact recall + reasoning + decision scenarios</p>"},{"location":"architecture/#5-self-improvement-loop-self_improve","title":"5. Self-Improvement Loop (<code>self_improve/</code>)","text":"<p>An 8-phase automated improvement cycle:</p> <pre><code>EVAL -&gt; ANALYZE -&gt; PROPOSE -&gt; CHALLENGE -&gt; VOTE -&gt; APPLY -&gt; RE-EVAL -&gt; DECIDE\n  |                                                                      |\n  +----------------------------------------------------------------------+\n                         (iterate up to N times)\n</code></pre> <p>Three cooperating modules: - runner.py -- orchestrates the 8 phases, manages iteration state, detects regression - patch_proposer.py -- LLM-powered analysis of failures, generates unified diffs - reviewer_voting.py -- devil's advocate challenge + 3-reviewer (quality, regression, simplicity) voting</p>"},{"location":"architecture/#6-multi-seed-evaluation-coremulti_seedpy","title":"6. Multi-Seed Evaluation (<code>core/multi_seed.py</code>)","text":"<p>Runs the same eval across multiple random seeds (default: 42, 123, 456, 789) to: - Measure inter-seed variance for each category - Flag noisy questions (&gt;10 percentage point variance) - Compute confidence intervals (mean +/- stddev)</p> <p>Each seed gets a fresh agent instance to avoid cross-contamination.</p>"},{"location":"architecture/#7-multi-agent-evaluation-pipeline-multi_agent_eval","title":"7. Multi-Agent Evaluation Pipeline (<code>multi_agent_eval/</code>)","text":"<p>An advanced evaluation pipeline using specialized agent roles: - GraderAgent -- grades from specific perspectives (quality, regression, simplicity) - AdversaryAgent -- generates difficult questions targeting known weaknesses - AnalystAgent -- analyzes results and proposes improvements - EvalCoordinator -- orchestrates the multi-agent pipeline - MultiAgentEvalPipeline -- end-to-end pipeline with adversarial rounds</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ol> <li> <p>Agent-agnostic: The <code>AgentAdapter</code> interface makes any agent evaluable. No assumptions about the agent's internal architecture.</p> </li> <li> <p>Deterministic data generation: Same seed always produces identical dialogues and questions. No LLM needed for data generation -- all content is template-based.</p> </li> <li> <p>Hybrid grading: Deterministic rubrics for fast, cheap, reproducible scoring of factual accuracy and specificity. LLM semantic judgment for nuanced dimensions (temporal awareness, source attribution, confidence calibration).</p> </li> <li> <p>Multi-vote stability: Grading N times and taking the median reduces LLM noise. For deterministic dimensions, multi-vote has zero overhead (same result every time).</p> </li> <li> <p>Progressive difficulty: L1 (simple recall) through L16 (decision-from-memory). Each level isolates a specific cognitive capability.</p> </li> <li> <p>Safety-gated self-improvement: The self-improvement loop never modifies the grader, test data, or safety constraints. Auto-revert on regression protects existing quality.</p> </li> <li> <p>Reproducibility: Full configuration logging, JSON report output, and seeded generation ensure any result can be reproduced.</p> </li> <li> <p>Zero external dependencies for core: The core package has no required dependencies. LLM grading requires <code>anthropic</code> (optional). The LearningAgent adapter requires <code>amplihack</code> (optional).</p> </li> </ol>"},{"location":"architecture/#data-flow-summary","title":"Data Flow Summary","text":"<pre><code>                    +-----------+\n                    |   User    |\n                    +-----+-----+\n                          |\n              CLI (amplihack-eval run)\n              or Python API (EvalRunner)\n                          |\n                    +-----v-----+\n                    | EvalRunner |\n                    +-----+-----+\n                          |\n            +-------------+-------------+\n            |                           |\n    +-------v--------+        +--------v--------+\n    | generate_dialogue|       | generate_questions|\n    | (long_horizon)  |       | (long_horizon)   |\n    +-------+--------+        +--------+--------+\n            |                          |\n    GroundTruth                 list[Question]\n    (turns with facts)          (with rubrics)\n            |                          |\n    +-------v--------+                |\n    | agent.learn()  |                |\n    | (N turns)      |                |\n    +-------+--------+                |\n            |                         |\n    +-------v-------------------------v--------+\n    |          agent.answer(question)           |\n    +-------+----------------------------------+\n            |\n    +-------v--------+\n    | _grade_hybrid() |  deterministic + LLM\n    | _grade_multi_vote() | N votes, median\n    +-------+--------+\n            |\n    +-------v--------+\n    |   EvalReport   |\n    | (scores by     |\n    |  category and  |\n    |  dimension)    |\n    +----------------+\n</code></pre>"},{"location":"architecture/#environment-variables","title":"Environment Variables","text":"Variable Purpose Default <code>ANTHROPIC_API_KEY</code> Required for LLM grading (none -- grading disabled) <code>GRADER_MODEL</code> LLM model for grading <code>claude-sonnet-4-5-20250929</code> <code>EVAL_MODEL</code> LLM model for LearningAgent adapter <code>claude-sonnet-4-5-20250929</code>"},{"location":"levels/","title":"Evaluation Levels (L1-L12)","text":""},{"location":"levels/#overview","title":"Overview","text":"<p>The evaluation framework uses 12 progressive difficulty levels to test different cognitive capabilities of AI agents. Each level builds on previous ones, requiring increasingly sophisticated reasoning.</p>"},{"location":"levels/#level-categories","title":"Level Categories","text":""},{"location":"levels/#core-levels-l1-l6","title":"Core Levels (L1-L6)","text":"<p>These test fundamental memory and reasoning capabilities.</p>"},{"location":"levels/#l1-single-source-direct-recall","title":"L1: Single Source Direct Recall","text":"<ul> <li>Tests: Basic fact retrieval from a single source</li> <li>Reasoning type: <code>direct_recall</code></li> <li>Example: \"How many total medals does Norway have?\" (answer is directly stated in one article)</li> <li>What it measures: Can the agent store and retrieve individual facts accurately?</li> </ul>"},{"location":"levels/#l2-multi-source-synthesis","title":"L2: Multi-Source Synthesis","text":"<ul> <li>Tests: Combining information across multiple independent sources</li> <li>Reasoning type: <code>cross_source_synthesis</code>, <code>specific_source_attribution</code></li> <li>Example: \"How many total goals were scored across all matches?\" (requires adding numbers from multiple articles)</li> <li>What it measures: Can the agent integrate information from different sources?</li> </ul>"},{"location":"levels/#l3-temporal-reasoning","title":"L3: Temporal Reasoning","text":"<ul> <li>Tests: Understanding changes over time, computing differences</li> <li>Reasoning type: <code>temporal_comparison</code>, <code>temporal_computation</code></li> <li>Requires: <code>temporal_ordering = True</code></li> <li>Example: \"How much did the stock price change between the two reports?\" (requires computing a delta)</li> <li>What it measures: Can the agent track temporal sequences and reason about changes?</li> </ul>"},{"location":"levels/#l4-procedural-learning","title":"L4: Procedural Learning","text":"<ul> <li>Tests: Learning and applying step-by-step procedures</li> <li>Reasoning type: <code>procedure_recall</code>, <code>conditional_procedure</code></li> <li>Example: \"What are the steps for the basic sourdough method?\" (requires recalling a sequence)</li> <li>What it measures: Can the agent learn and reproduce procedures?</li> </ul>"},{"location":"levels/#l5-contradiction-handling","title":"L5: Contradiction Handling","text":"<ul> <li>Tests: Detecting and reasoning about conflicting information</li> <li>Reasoning type: <code>contradiction_identification</code>, <code>contradiction_resolution</code></li> <li>Example: \"What are the conflicting claims about the vaccine efficacy?\" (two sources disagree)</li> <li>What it measures: Can the agent detect contradictions and reason about them?</li> </ul>"},{"location":"levels/#l6-incremental-learning","title":"L6: Incremental Learning","text":"<ul> <li>Tests: Updating knowledge when new information arrives</li> <li>Reasoning type: <code>knowledge_update</code>, <code>history_awareness</code></li> <li>Requires: <code>update_handling = True</code></li> <li>Example: \"What is the current CEO after the leadership change?\" (requires updating a stored fact)</li> <li>What it measures: Can the agent update beliefs when new information supersedes old?</li> </ul>"},{"location":"levels/#teacher-student-l7","title":"Teacher-Student (L7)","text":""},{"location":"levels/#l7-teaching-session","title":"L7: Teaching Session","text":"<ul> <li>Tests: Agent learns material, then teaches it; graded on teaching accuracy</li> <li>Reasoning type: <code>teaching_accuracy</code>, <code>self_explanation</code></li> <li>Example: Agent learns about photosynthesis, then must explain it clearly enough for a student to understand</li> <li>What it measures: Depth of understanding (the best test of knowledge is teaching it)</li> </ul>"},{"location":"levels/#advanced-levels-l8-l10","title":"Advanced Levels (L8-L10)","text":"<p>These test metacognitive and causal reasoning.</p>"},{"location":"levels/#l8-confidence-calibration","title":"L8: Confidence Calibration","text":"<ul> <li>Tests: Knowing what you know vs. what you do not know</li> <li>Reasoning type: <code>confidence_calibration</code>, <code>uncertainty_detection</code></li> <li>Example: \"How confident are you about X?\" (agent should be uncertain when info is ambiguous)</li> <li>What it measures: Is the agent's confidence well-calibrated to its actual accuracy?</li> </ul>"},{"location":"levels/#l9-causal-reasoning","title":"L9: Causal Reasoning","text":"<ul> <li>Tests: Identifying causal chains and root causes</li> <li>Reasoning type: <code>causal_chain</code>, <code>root_cause</code></li> <li>Example: \"What caused the server outage?\" (requires tracing a chain of events)</li> <li>What it measures: Can the agent identify cause-and-effect relationships?</li> </ul>"},{"location":"levels/#l10-counterfactual-reasoning","title":"L10: Counterfactual Reasoning","text":"<ul> <li>Tests: \"What if X didn't happen?\" reasoning</li> <li>Reasoning type: <code>counterfactual</code>, <code>alternative_outcome</code></li> <li>Example: \"What would have happened if the backup system had worked?\" (requires hypothetical reasoning)</li> <li>What it measures: Can the agent reason about alternative scenarios?</li> </ul>"},{"location":"levels/#novel-skills-l11-l12","title":"Novel Skills (L11-L12)","text":"<p>These test the agent's ability to learn genuinely new capabilities.</p>"},{"location":"levels/#l11-novel-skill-acquisition","title":"L11: Novel Skill Acquisition","text":"<ul> <li>Tests: Learning genuinely new skills from documentation</li> <li>Reasoning type: <code>skill_application</code>, <code>novel_syntax</code></li> <li>Example: Learn a made-up programming syntax, then write code in it</li> <li>What it measures: Can the agent learn and apply truly novel skills?</li> </ul>"},{"location":"levels/#l12-far-transfer","title":"L12: Far Transfer","text":"<ul> <li>Tests: Applying learned reasoning patterns to new domains</li> <li>Reasoning type: <code>cross_domain_transfer</code>, <code>analogical_reasoning</code></li> <li>Example: Learn a pattern in domain A, apply it to solve a problem in domain B</li> <li>What it measures: Can the agent abstract and transfer knowledge across domains?</li> </ul>"},{"location":"levels/#data-structure","title":"Data Structure","text":"<p>Each level is defined as a <code>TestLevel</code> dataclass:</p> <pre><code>@dataclass\nclass TestLevel:\n    level_id: str                       # \"L1\", \"L2\", etc.\n    level_name: str                     # Human-readable name\n    description: str                    # What the level tests\n    articles: list[TestArticle]         # Source content\n    questions: list[TestQuestion]       # Evaluation questions\n    requires_temporal_ordering: bool    # Does order matter?\n    requires_update_handling: bool      # Does info get superseded?\n</code></pre>"},{"location":"levels/#accessing-levels-programmatically","title":"Accessing Levels Programmatically","text":"<pre><code>from amplihack_eval.levels import ALL_LEVELS, get_level_by_id\n\n# Get all core levels (L1-L6)\nfor level in ALL_LEVELS:\n    print(f\"{level.level_id}: {level.level_name} ({len(level.questions)} questions)\")\n\n# Get a specific level\nl3 = get_level_by_id(\"L3\")\nprint(f\"L3 has {len(l3.articles)} articles and {len(l3.questions)} questions\")\n\n# Access level groups\nfrom amplihack_eval.levels import (\n    TEACHER_STUDENT_LEVELS,  # [L7]\n    ADVANCED_LEVELS,         # [L8, L9, L10]\n    NOVEL_SKILL_LEVELS,      # [L11]\n    TRANSFER_LEVELS,         # [L12]\n)\n</code></pre>"},{"location":"levels/#adding-new-levels","title":"Adding New Levels","text":"<p>To add a new evaluation level (e.g., L13):</p> <ol> <li>Define the level in <code>src/amplihack_eval/data/progressive_levels.py</code>:</li> </ol> <pre><code>LEVEL_13 = TestLevel(\n    level_id=\"L13\",\n    level_name=\"Your Level Name\",\n    description=\"What this level tests\",\n    articles=[\n        TestArticle(\n            title=\"Source Article\",\n            content=\"Article content with facts...\",\n            url=\"https://example.com/article\",\n            published=\"2026-01-01T00:00:00Z\",\n        )\n    ],\n    questions=[\n        TestQuestion(\n            question=\"A question about the content\",\n            expected_answer=\"The expected answer\",\n            level=\"L13\",\n            reasoning_type=\"your_reasoning_type\",\n        )\n    ],\n)\n</code></pre> <ol> <li>Add it to the appropriate level group list</li> <li>Re-export it from <code>src/amplihack_eval/levels/__init__.py</code></li> <li>Add tests in <code>tests/test_data_generation.py</code></li> </ol>"},{"location":"multi-agent-eval/","title":"Multi-Agent Evaluation","text":""},{"location":"multi-agent-eval/#status","title":"Status","text":"<p>The multi-agent evaluation module (<code>amplihack_eval.multi_agent_eval</code>) is reserved for future development. The module exists as a placeholder with plans for the following capabilities.</p>"},{"location":"multi-agent-eval/#planned-architecture","title":"Planned Architecture","text":"<p>Multi-agent evaluation will test scenarios where multiple agents collaborate or compete to accomplish tasks. Unlike single-agent evaluation (which tests memory and reasoning), multi-agent evaluation tests coordination, communication, and role specialization.</p>"},{"location":"multi-agent-eval/#planned-scenarios","title":"Planned Scenarios","text":""},{"location":"multi-agent-eval/#collaborative-knowledge-building","title":"Collaborative Knowledge Building","text":"<p>Multiple agents each learn different subsets of information, then must combine their knowledge to answer questions that no single agent could answer alone.</p> <pre><code>Agent A learns: Articles 1-5\nAgent B learns: Articles 6-10\nAgent C learns: Articles 11-15\n\nQuestion: \"Compare findings from Article 3 and Article 12\"\n-&gt; Requires A and C to collaborate\n</code></pre>"},{"location":"multi-agent-eval/#debate-and-consensus","title":"Debate and Consensus","text":"<p>Agents are given ambiguous or contradictory information and must debate to reach a consensus answer. Tests argumentation quality, evidence weighing, and convergence.</p>"},{"location":"multi-agent-eval/#task-delegation","title":"Task Delegation","text":"<p>A coordinator agent receives a complex task and must delegate subtasks to specialist agents, then synthesize their results. Tests planning, delegation, and integration.</p>"},{"location":"multi-agent-eval/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>One agent attempts to inject misleading information while others must maintain accuracy. Tests resilience to adversarial inputs.</p>"},{"location":"multi-agent-eval/#planned-interface","title":"Planned Interface","text":"<p>The multi-agent adapter interface will extend <code>AgentAdapter</code>:</p> <pre><code>class MultiAgentAdapter(AgentAdapter):\n    \"\"\"Adapter for a group of agents that can communicate.\"\"\"\n\n    @abstractmethod\n    def send_message(self, from_agent: str, to_agent: str, message: str) -&gt; str:\n        \"\"\"Send a message between agents.\"\"\"\n\n    @abstractmethod\n    def get_agents(self) -&gt; list[str]:\n        \"\"\"List all agent identifiers in the group.\"\"\"\n\n    @abstractmethod\n    def assign_role(self, agent_id: str, role: str) -&gt; None:\n        \"\"\"Assign a role to a specific agent.\"\"\"\n</code></pre>"},{"location":"multi-agent-eval/#contributing","title":"Contributing","text":"<p>If you are interested in contributing to the multi-agent evaluation module, please open an issue on GitHub to discuss your proposed scenario before implementing.</p>"},{"location":"self-improvement/","title":"Self-Improvement Loop","text":""},{"location":"self-improvement/#overview","title":"Overview","text":"<p>The self-improvement loop automates the process of identifying evaluation weaknesses and iteratively improving agent performance. It follows a disciplined 8-phase cycle with safety gates at every step, inspired by the principle: measure first, change second.</p> <p>The loop consists of three cooperating modules:</p> <ul> <li><code>runner.py</code> -- Orchestrates the 8 phases, manages iteration state, detects regression</li> <li><code>patch_proposer.py</code> -- LLM-powered analysis of failures, generates hypotheses and unified diffs</li> <li><code>reviewer_voting.py</code> -- Devil's advocate challenge + 3-reviewer voting (quality, regression, simplicity)</li> </ul>"},{"location":"self-improvement/#the-8-phase-improvement-cycle","title":"The 8-Phase Improvement Cycle","text":"<pre><code>EVAL -&gt; ANALYZE -&gt; PROPOSE -&gt; CHALLENGE -&gt; VOTE -&gt; APPLY -&gt; RE-EVAL -&gt; DECIDE\n  |                                                                      |\n  +----------------------------------------------------------------------+\n                           (iterate up to N times)\n</code></pre>"},{"location":"self-improvement/#phase-1-eval","title":"Phase 1: EVAL","text":"<p>Run the full long-horizon evaluation to get per-category scores.</p> <pre><code>evaluator = EvalRunner(num_turns=config.num_turns, num_questions=config.num_questions, seed=config.seed)\nreport = evaluator.run(agent, grader_model=config.grader_model)\n</code></pre> <p>A fresh agent is created from the <code>agent_factory</code> callable at each iteration to ensure clean state. The <code>EvalReport</code> provides <code>overall_score</code>, <code>category_breakdown</code> (per-category averages, min, max, dimension averages), and individual <code>EvalResult</code> entries for every question.</p>"},{"location":"self-improvement/#phase-2-analyze","title":"Phase 2: ANALYZE","text":"<p>Identify the worst-performing category and diagnose the bottleneck.</p> <p>The <code>_analyze_categories()</code> function groups all questions by category, identifies questions scoring below <code>failure_threshold</code>, and calls <code>_diagnose_bottleneck()</code> to map the failure to a specific system component.</p> <p>Bottleneck Diagnosis Mapping:</p> <p>The framework maps categories to system components using a two-level approach: category-specific mapping first, then fallback to worst-dimension mapping.</p> Category Bottleneck Component Suggested Fix Direction <code>needle_in_haystack</code> <code>retrieval:keyword_search</code> Entity-centric indexing for people/projects <code>meta_memory</code> <code>retrieval:aggregation</code> Route \"how many\" / \"list all\" to COUNT/DISTINCT queries <code>source_attribution</code> <code>retrieval:source_tracking</code> Ensure source_label is included in retrieval results <code>temporal_evolution</code> <code>retrieval:temporal_ordering</code> Ensure temporal_index metadata for chronological sorting <code>cross_reference</code> <code>retrieval:graph_traversal</code> Expand hop depth to connect facts across blocks <code>numerical_precision</code> <code>synthesis:arithmetic</code> Ensure calculate tool is used for all math operations <code>distractor_resistance</code> <code>retrieval:confidence_weighting</code> Deprioritize distractor blocks with lower confidence <p>Dimension-based fallback (when no category-specific mapping exists):</p> Worst Dimension Bottleneck Component Suggested Fix <code>factual_accuracy</code> <code>retrieval:coverage</code> Increase retrieval coverage <code>specificity</code> <code>retrieval:precision</code> Improve retrieval precision <code>temporal_awareness</code> <code>retrieval:temporal</code> Add temporal metadata <code>source_attribution</code> <code>retrieval:provenance</code> Improve source tracking <code>confidence_calibration</code> <code>synthesis:calibration</code> Improve confidence expression <p>Categories are sorted by average score (worst first). If no categories are below <code>failure_threshold</code>, the loop stops early with a \"all categories above threshold\" message.</p>"},{"location":"self-improvement/#phase-3-propose","title":"Phase 3: PROPOSE","text":"<p>Use LLM to generate a patch hypothesis targeting the worst category's bottleneck.</p> <p>The <code>propose_patch()</code> function in <code>patch_proposer.py</code>:</p> <ol> <li> <p>Reads the target file based on the bottleneck component identifier. A <code>component_file_map</code> can be provided to map component prefixes (e.g., <code>\"retrieval:\"</code>) to file paths.</p> </li> <li> <p>Builds a structured prompt containing:</p> </li> <li>The failing category name, score, bottleneck, and suggested fix direction</li> <li>Up to 5 failed questions with their expected vs. actual answers, scores, and per-dimension breakdowns</li> <li>Previously reverted patches (with revert reasons) -- to prevent repeating failed fixes</li> <li>Previously rejected patches (with rejection reasons) -- to force different approaches</li> <li> <p>The target file's source code (up to 3000 characters)</p> </li> <li> <p>Calls the LLM to generate a structured JSON response containing:</p> </li> <li><code>hypothesis</code> -- why the category is failing</li> <li><code>description</code> -- what the patch does</li> <li><code>diff</code> -- unified diff format of the proposed change</li> <li><code>expected_impact</code> -- mapping of category name to expected score delta in percentage points</li> <li><code>risk_assessment</code> -- what could go wrong</li> <li> <p><code>confidence</code> -- 0.0 to 1.0</p> </li> <li> <p>Returns a <code>PatchProposal</code> dataclass. If no LLM callable is provided, a stub proposal with 10% confidence is returned.</p> </li> </ol> <p>Patch rules enforced by the prompt:</p> <ul> <li>The diff must be valid unified diff format</li> <li>Focus on the SMALLEST change that addresses the root cause</li> <li>Do NOT change test infrastructure, graders, or eval harness</li> <li>Prefer prompt/instruction changes over algorithmic changes</li> <li>Be honest about confidence -- lower is better than overconfident</li> </ul>"},{"location":"self-improvement/#phase-4-challenge","title":"Phase 4: CHALLENGE","text":"<p>A devil's advocate reviews the proposal and raises concerns.</p> <p>The <code>challenge_proposal()</code> function in <code>reviewer_voting.py</code> runs a two-step process:</p> <p>Step 1 -- Devil's Advocate Attack:</p> <p>An LLM call with the <code>DEVIL_ADVOCATE_PROMPT</code> argues aggressively against the proposal: - What assumptions could be wrong? - What could this break? - Is there a simpler alternative? - Is the hypothesis even correct?</p> <p>The response includes a list of arguments against and a worst-case scenario.</p> <p>Step 2 -- Proposer Defense:</p> <p>A second LLM call receives the challenge arguments and the original proposal, then responds with: - <code>defense</code> -- why the patch should still be applied - <code>concerns_acknowledged</code> -- valid concerns the proposer acknowledges - <code>concerns_refuted</code> -- concerns the proposer has addressed</p> <p>Adequacy Check:</p> <p>Concerns are considered adequately addressed if the proposer responds to at least 50% of the challenge arguments (acknowledged + refuted &gt;= 50% of total challenges). If concerns are NOT adequately addressed, the proposal is logged as rejected and the iteration skips to the next cycle.</p>"},{"location":"self-improvement/#phase-5-vote","title":"Phase 5: VOTE","text":"<p>Three independent reviewers each vote on the proposal. Each reviewer has a distinct perspective:</p> <p>Quality Reviewer -- evaluates: - Engineering best practices (clean code, no side effects) - Proper error handling - Appropriate use of abstractions - Consistency with existing code style - Whether the change is testable</p> <p>Regression Reviewer -- evaluates: - Could this change break OTHER categories that currently pass? - Does it modify shared code paths affecting unrelated functionality? - Are there edge cases causing unexpected failures? - Is the change scoped narrowly enough to avoid collateral damage?</p> <p>Simplicity Reviewer -- evaluates: - Is this the SIMPLEST possible fix? - Could a smaller change achieve the same result? - Does it add unnecessary complexity or abstraction? - Could a prompt-only change work instead of a code change?</p> <p>Each reviewer receives the formatted proposal (including the challenge phase results if available) and responds with a structured JSON vote: <code>accept</code>, <code>reject</code>, or <code>modify</code>, along with a rationale and specific concerns.</p> <p>Vote Tallying:</p> <pre><code>&gt;50% accept  -&gt; \"accepted\"\n&gt;50% reject  -&gt; \"rejected\"\notherwise    -&gt; \"modified\" (mixed signals)\n</code></pre> <p>With 3 reviewers, this means at least 2 must accept for the proposal to pass.</p> <p>If the proposal is rejected, it is logged in <code>PatchHistory.rejected_patches</code> and the iteration continues to the next cycle.</p>"},{"location":"self-improvement/#phase-6-apply","title":"Phase 6: APPLY","text":"<p>If the majority votes accept, the patch is applied. The proposal is logged in <code>PatchHistory.applied_patches</code> with its target file, description, hypothesis, and confidence.</p>"},{"location":"self-improvement/#phase-7-re-eval","title":"Phase 7: RE-EVAL","text":"<p>Run the evaluation again with a fresh agent to measure the patch's impact. The post-evaluation scores are compared against the baseline scores from Phase 1.</p>"},{"location":"self-improvement/#phase-8-decide","title":"Phase 8: DECIDE","text":"<p>The <code>detect_regression()</code> function checks whether any individual category regressed beyond the configured threshold:</p> <pre><code>def detect_regression(\n    baseline_scores: dict[str, float],\n    post_scores: dict[str, float],\n    threshold: float = 5.0,\n) -&gt; tuple[bool, str, float]:\n</code></pre> <p>Regression Detection Logic:</p> <ol> <li>Compute the delta (in percentage points) for each category: <code>baseline - post</code></li> <li>Find the maximum regression across all categories</li> <li>Find the maximum gain across all categories</li> <li>Regression is detected if: <code>max_regression &gt; threshold AND max_gain &lt; threshold</code></li> </ol> <p>This means a large gain in one category can compensate for a small regression in another, but a large regression without corresponding gains triggers a revert.</p> <p>If regression detected: The patch is automatically reverted and logged in <code>PatchHistory.reverted_patches</code> with the revert reason.</p> <p>If no regression: The patch is kept and the loop continues.</p>"},{"location":"self-improvement/#configuration","title":"Configuration","text":"<pre><code>from amplihack_eval.self_improve.runner import SelfImproveConfig\n\nconfig = SelfImproveConfig(\n    num_turns=100,              # Dialogue turns per evaluation\n    num_questions=20,           # Questions per evaluation\n    seed=42,                    # Random seed for reproducibility\n    max_iterations=3,           # Maximum improvement iterations\n    failure_threshold=0.7,      # Scores below this are \"failures\"\n    regression_threshold=5.0,   # Max regression (pp) before auto-revert\n    output_dir=\"/tmp/self-improve\",  # Where to write logs and reports\n    grader_model=\"\",            # LLM model for grading (default if empty)\n)\n</code></pre> <p>Parameter Tuning Guidelines:</p> Parameter Conservative Moderate Aggressive <code>failure_threshold</code> 0.8 0.7 0.5 <code>regression_threshold</code> 2.0 5.0 10.0 <code>max_iterations</code> 3 5 10 <code>num_turns</code> 100 500 1000 <code>num_questions</code> 20 50 100 <ul> <li>Conservative: Strict quality gates, fewer iterations, catches only severe failures</li> <li>Moderate: Balanced -- good default for most use cases</li> <li>Aggressive: Allows more latitude, more iterations, may find more improvements but risks more churn</li> </ul>"},{"location":"self-improvement/#key-data-structures","title":"Key Data Structures","text":""},{"location":"self-improvement/#patchproposal","title":"PatchProposal","text":"<pre><code>@dataclass\nclass PatchProposal:\n    target_file: str                       # Path to the file to modify\n    hypothesis: str                        # Why the category is failing\n    description: str                       # What the patch does\n    diff: str                              # Unified diff format\n    expected_impact: dict[str, float] = {} # category -&gt; expected score delta (pp)\n    risk_assessment: str = \"\"              # What could go wrong\n    confidence: float = 0.0               # 0.0 to 1.0\n</code></pre>"},{"location":"self-improvement/#patchhistory","title":"PatchHistory","text":"<pre><code>@dataclass\nclass PatchHistory:\n    applied_patches: list[dict] = []     # Successfully applied patches\n    reverted_patches: list[dict] = []    # Patches that caused regression\n    rejected_patches: list[dict] = []    # Patches rejected by reviewers\n</code></pre> <p>The history is passed to the patch proposer at each iteration. Previously reverted and rejected patches are included in the LLM prompt with their failure reasons, preventing the system from proposing the same fix twice.</p>"},{"location":"self-improvement/#categoryanalysis","title":"CategoryAnalysis","text":"<pre><code>@dataclass\nclass CategoryAnalysis:\n    category: str                          # Category name\n    avg_score: float                       # Average score\n    num_questions: int                     # Number of questions\n    failed_questions: list[dict] = []      # Details of failed questions\n    bottleneck: str = \"\"                   # Identified system component\n    suggested_fix: str = \"\"                # Suggested improvement direction\n</code></pre>"},{"location":"self-improvement/#reviewvote","title":"ReviewVote","text":"<pre><code>@dataclass\nclass ReviewVote:\n    reviewer_id: str                       # \"quality\", \"regression\", or \"simplicity\"\n    vote: str                              # \"accept\", \"reject\", or \"modify\"\n    rationale: str                         # Why this vote was cast\n    concerns: list[str] = []              # Specific concerns\n    suggested_modifications: str | None = None\n</code></pre>"},{"location":"self-improvement/#challengeresponse","title":"ChallengeResponse","text":"<pre><code>@dataclass\nclass ChallengeResponse:\n    challenge_arguments: list[str]         # Arguments against the patch\n    proposer_response: str                 # The proposer's defense\n    concerns_addressed: bool               # Were &gt;= 50% of concerns addressed?\n    remaining_concerns: list[str] = []     # Unaddressed concerns\n</code></pre>"},{"location":"self-improvement/#reviewresult","title":"ReviewResult","text":"<pre><code>@dataclass\nclass ReviewResult:\n    proposal: PatchProposal                # The proposal being reviewed\n    challenge: ChallengeResponse | None    # Challenge phase results\n    votes: list[ReviewVote]                # Individual reviewer votes\n    decision: str                          # \"accepted\", \"rejected\", or \"modified\"\n    consensus_rationale: str               # Summary rationale\n</code></pre>"},{"location":"self-improvement/#runnerresult","title":"RunnerResult","text":"<pre><code>@dataclass\nclass RunnerResult:\n    config: dict[str, Any]                 # Configuration used\n    iterations: list[IterationResult]      # Per-iteration results\n    score_progression: list[float]         # Overall score at each iteration\n    category_progression: dict[str, list[float]]  # Per-category scores over time\n    total_duration_seconds: float\n</code></pre>"},{"location":"self-improvement/#iterationresult","title":"IterationResult","text":"<pre><code>@dataclass\nclass IterationResult:\n    iteration: int                         # Iteration number (1-based)\n    report: dict[str, Any]                 # Full EvalReport as dict\n    category_analyses: list[dict]          # Per-category failure analysis\n    improvements_applied: list[str]        # Descriptions of applied patches\n    patch_proposal: dict | None = None     # Proposal details\n    review_result: dict | None = None      # Review details\n    post_scores: dict[str, float] | None = None  # Post-eval scores\n    reverted: bool = False                 # Was this patch reverted?\n    revert_reason: str = \"\"                # Why it was reverted\n    duration_seconds: float = 0.0          # Iteration wall clock time\n</code></pre>"},{"location":"self-improvement/#safety-constraints","title":"Safety Constraints","text":"<p>The self-improvement loop enforces strict safety rules:</p> <ol> <li> <p>Never modifies the grader: The grading system is the source of truth. The LLM prompt explicitly forbids changes to test infrastructure, graders, or eval harness.</p> </li> <li> <p>Never modifies test data: Test levels and questions are fixed. Data generation is deterministic and untouchable.</p> </li> <li> <p>Never weakens safety constraints: The challenge and voting phases cannot be circumvented. Even if an LLM proposes removing safety gates, reviewers would reject it.</p> </li> <li> <p>Auto-revert on regression: If the overall score or any category drops by more than <code>regression_threshold</code> percentage points, the patch is automatically reverted. This protects existing quality from well-intentioned but harmful changes.</p> </li> <li> <p>Full history prevents repetition: All attempted patches (applied, reverted, rejected) are tracked in <code>PatchHistory</code> and included in future proposal prompts, preventing the system from proposing the same failed fix repeatedly.</p> </li> <li> <p>Majority vote required: No patch is applied without at least 2/3 reviewers voting to accept.</p> </li> <li> <p>Devil's advocate gate: Proposals that fail the challenge phase (less than 50% of concerns addressed) are rejected before reaching the voting stage.</p> </li> </ol>"},{"location":"self-improvement/#cli-usage","title":"CLI Usage","text":"<pre><code># Run 5 iterations of self-improvement\namplihack-eval self-improve --iterations 5 --turns 100 --questions 20\n\n# Large-scale improvement with custom output\namplihack-eval self-improve --iterations 10 --turns 500 --questions 50 --output-dir ./logs\n\n# Conservative settings (strict regression threshold)\namplihack-eval self-improve --iterations 3 --regression-threshold 2.0\n</code></pre>"},{"location":"self-improvement/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from amplihack_eval.self_improve.runner import SelfImproveConfig, run_self_improve\nfrom amplihack_eval.adapters.learning_agent import LearningAgentAdapter\n\nconfig = SelfImproveConfig(\n    num_turns=100,\n    num_questions=20,\n    max_iterations=5,\n    failure_threshold=0.7,\n    regression_threshold=5.0,\n    output_dir=\"/tmp/self-improve\",\n)\n\n# Agent factory creates a fresh agent for each iteration\ndef make_agent():\n    return LearningAgentAdapter(model=\"claude-sonnet-4-5-20250929\")\n\n# Optional: provide an LLM callable for patch proposal and review\nimport anthropic\nclient = anthropic.Anthropic()\n\ndef llm_call(prompt: str) -&gt; str:\n    response = client.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    return response.content[0].text\n\n# Run the loop\nresult = run_self_improve(\n    config=config,\n    agent_factory=make_agent,\n    llm_call=llm_call,\n    project_root=Path(\"/path/to/agent/code\"),\n)\n\n# Inspect results\nprint(f\"Iterations: {len(result.iterations)}\")\nprint(f\"Score progression: {' -&gt; '.join(f'{s:.2%}' for s in result.score_progression)}\")\nprint(f\"Duration: {result.total_duration_seconds:.1f}s\")\n\nfor it in result.iterations:\n    status = \"REVERTED\" if it.reverted else \"KEPT\"\n    print(f\"  Iteration {it.iteration}: {status}\")\n    if it.patch_proposal:\n        print(f\"    Patch: {it.patch_proposal.get('description', '')[:80]}\")\n    if it.review_result:\n        print(f\"    Review: {it.review_result.get('decision', '')}\")\n</code></pre>"},{"location":"self-improvement/#output-structure","title":"Output Structure","text":"<p>Each run produces files in the output directory:</p> <pre><code>/tmp/self-improve/\n    self_improve_summary.json       # High-level summary\n    iteration_1/\n        report.json                 # Full EvalReport from Phase 1\n    iteration_2/\n        report.json\n    ...\n</code></pre>"},{"location":"self-improvement/#summary-json","title":"Summary JSON","text":"<pre><code>{\n    \"config\": {\n        \"num_turns\": 100,\n        \"num_questions\": 20,\n        \"max_iterations\": 5,\n        \"failure_threshold\": 0.7,\n        \"regression_threshold\": 5.0\n    },\n    \"score_progression\": [0.72, 0.78, 0.82, 0.85],\n    \"category_progression\": {\n        \"needle_in_haystack\": [0.88, 0.90, 0.92, 0.92],\n        \"temporal_evolution\": [0.55, 0.68, 0.72, 0.78],\n        \"numerical_precision\": [0.62, 0.65, 0.70, 0.75]\n    },\n    \"total_duration_seconds\": 342.5,\n    \"iterations_run\": 4,\n    \"patches_applied\": 3,\n    \"patches_reverted\": 1,\n    \"patches_rejected\": 0\n}\n</code></pre>"},{"location":"self-improvement/#how-it-works-end-to-end","title":"How It Works End-to-End","text":"<p>A typical 5-iteration run:</p> <pre><code>Iteration 1:\n  EVAL:      Overall 72%, temporal_evolution at 55% (worst)\n  ANALYZE:   Bottleneck: retrieval:temporal_ordering\n  PROPOSE:   \"Add temporal_index metadata to all time-series facts\"\n  CHALLENGE: 3 arguments raised, 2 addressed, 1 remaining -&gt; PASS\n  VOTE:      quality=accept, regression=accept, simplicity=modify -&gt; ACCEPTED\n  APPLY:     Patch applied\n  RE-EVAL:   temporal_evolution 55% -&gt; 68%, overall 72% -&gt; 78%\n  DECIDE:    No regression -&gt; KEEP\n\nIteration 2:\n  EVAL:      Overall 78%, numerical_precision at 62% (worst)\n  ANALYZE:   Bottleneck: synthesis:arithmetic\n  PROPOSE:   \"Route math questions to calculate tool\"\n  CHALLENGE: 2 arguments raised, 2 addressed -&gt; PASS\n  VOTE:      quality=accept, regression=reject, simplicity=accept -&gt; ACCEPTED\n  APPLY:     Patch applied\n  RE-EVAL:   numerical_precision 62% -&gt; 70%, but source_attribution 80% -&gt; 72%\n  DECIDE:    source_attribution regressed 8pp &gt; 5pp threshold -&gt; REVERT\n\nIteration 3:\n  EVAL:      Overall 78% (reverted to pre-iteration-2 state)\n  ANALYZE:   numerical_precision at 62% (same as before, different approach needed)\n  PROPOSE:   \"Add arithmetic verification step to synthesis prompt\"\n             (History shows previous approach was reverted due to regression)\n  CHALLENGE: 2 arguments raised, 2 addressed -&gt; PASS\n  VOTE:      quality=accept, regression=accept, simplicity=accept -&gt; ACCEPTED\n  APPLY:     Patch applied\n  RE-EVAL:   numerical_precision 62% -&gt; 73%, no regressions\n  DECIDE:    No regression -&gt; KEEP\n</code></pre>"},{"location":"self-improvement/#real-world-results","title":"Real-World Results","text":"<p>From the amplihack development history (5-loop improvement cycle):</p> <ul> <li>Starting score: 83.2%</li> <li>Final score: 96.6% (+13.4 percentage points)</li> <li>Biggest single improvement: Source-specific fact filtering, +53.3% on L2 (multi-source synthesis)</li> <li>Key insight: Retrieval threshold of 50 was too low; increasing to 150 prevented cascading failures as the knowledge base grew</li> </ul>"},{"location":"self-improvement/#integration-with-multi-seed-evaluation","title":"Integration with Multi-Seed Evaluation","text":"<p>For more robust improvement decisions, combine self-improvement with multi-seed evaluation:</p> <pre><code>from amplihack_eval.core.multi_seed import run_multi_seed_eval\n\n# After each improvement iteration, validate across multiple seeds\nseeds = [42, 123, 456, 789]\nmulti_result = run_multi_seed_eval(agent, seeds=seeds, num_turns=100, num_questions=20)\n\n# Check if improvement is consistent across seeds (not just lucky on seed=42)\nif multi_result.inter_seed_variance[\"temporal_evolution\"] &gt; 0.10:\n    print(\"WARNING: High variance -- improvement may be seed-dependent\")\n</code></pre> <p>This guards against improvements that only work with specific random seeds.</p>"}]}